[ { "title": "End-to-End Testing for AI Agents with LLM as a Judge", "url": "/posts/llm-as-judge-e2e-testing-ai-agents/", "categories": "", "tags": "", "date": "2025-12-25 20:00:00 -0600", "snippet": "The Testing ChallengeAI agents are becoming critical infrastructure. From customer support to data analysis, organizations are deploying agents that make decisions, call APIs, and generate responses autonomously. But this raises a fundamental question: how do you test something that produces non-deterministic outputs?Traditional software testing relies on deterministic assertions. Given input X, expect output Y. But when an LLM is at the core of your system, the same prompt can produce semantically equivalent but textually different responses every time. Your agent might say ‚ÄúYou have 3 properties‚Äù today and ‚ÄúI found three listings for you‚Äù tomorrow. Both are correct, but assertEqual will fail.This post introduces the LLM-as-judge pattern, a technique we developed to solve this problem. We‚Äôll walk through how we built an end-to-end testing framework for our AI agent that uses a separate, more capable LLM to evaluate responses, enabling reliable automated testing of inherently non-deterministic systems.What is an AI Agent?Before diving into testing, let‚Äôs establish what we mean by an ‚Äúagent.‚Äù At its core, an AI agent consists of three components:1. The Harness (System Prompt)The harness defines the agent‚Äôs personality, constraints, and context. It‚Äôs the system prompt that shapes how the LLM behaves, what it knows about itself, and what rules it should follow.In production, this isn‚Äôt a simple paragraph. Our system prompt is a structured document spanning several hundred tokens, organized into sections:&amp;lt;identity&amp;gt;You are Neyoba, the AI revenue management assistant for Beyond...&amp;lt;/identity&amp;gt;&amp;lt;role&amp;gt;Help users make smarter pricing and performance decisions. Surface insights,explain data clearly, and guide users to appropriate actions...&amp;lt;/role&amp;gt;&amp;lt;constraints&amp;gt;- Cannot process file uploads or generate downloadable files- If listing lacks dynamic pricing, recommend enabling it first- Never suggest disabling Beyond pricing- Never use internal terminology...&amp;lt;/constraints&amp;gt;&amp;lt;formatting&amp;gt;- Links: inline HTML with target=&quot;_blank&quot;- Structure: bold headers, short paragraphs, markdown headings- Data: use tables for comparisons...&amp;lt;/formatting&amp;gt;&amp;lt;localization&amp;gt;- Spanish: formal &quot;usted&quot; unless user uses &quot;t√∫&quot;- Portuguese: Brazilian conventions- Dates/currency/spelling: match user&#39;s region...&amp;lt;/localization&amp;gt;&amp;lt;response-behavior&amp;gt; &amp;lt;ambiguous-questions&amp;gt; For broad questions (&quot;how am I doing?&quot;): 1. State your interpretation 2. Deliver most actionable insight first 3. Offer 2-3 specific follow-ups &amp;lt;/ambiguous-questions&amp;gt; &amp;lt;pacing-questions&amp;gt;...&amp;lt;/pacing-questions&amp;gt; &amp;lt;clarifications&amp;gt;...&amp;lt;/clarifications&amp;gt;&amp;lt;/response-behavior&amp;gt;&amp;lt;tool-usage&amp;gt;- Never call get_listing_details_by_id in loops- For 50+ listings, use market-level aggregates- Combine tools: get_listings for IDs, then get_bookings for details...&amp;lt;/tool-usage&amp;gt;The prompt covers identity, role definition, hard constraints, formatting rules, localization preferences, response length guidelines, multi-turn conversation handling, and tool usage patterns. This level of detail is essential for consistent, production-quality agent behavior.You can explore our full production system prompt on LangSmith Prompt Hub.2. The Reasoning Loop (ReAct Pattern)The agent uses a reasoning loop to decide what to do next. We implement the ReAct (Reasoning + Acting) pattern via LangGraph, where the LLM can: Reason about the user‚Äôs request Decide which tool to call Process tool results Generate a final response3. The Tools (MCP Server)Tools give the agent real capabilities. We use the Model Context Protocol (MCP) to expose tools via an SSE server. The agent can call tools like get_listings, get_bookings, or get_revenue_data to fetch real information from our database.Here‚Äôs how these components interact in a typical request:sequenceDiagram participant User participant Agent as ReactAgent participant LLM as LLM (GPT-5 Mini) participant MCP as MCP Server participant DB as Database User-&amp;gt;&amp;gt;Agent: &quot;What are my listings?&quot; Agent-&amp;gt;&amp;gt;LLM: System prompt + User message LLM-&amp;gt;&amp;gt;Agent: Tool call: get_listings(token) Agent-&amp;gt;&amp;gt;MCP: Execute tool via SSE MCP-&amp;gt;&amp;gt;DB: Query listings DB-&amp;gt;&amp;gt;MCP: Listing data MCP-&amp;gt;&amp;gt;Agent: Tool result Agent-&amp;gt;&amp;gt;LLM: Tool result + context LLM-&amp;gt;&amp;gt;Agent: Final response Agent-&amp;gt;&amp;gt;User: &quot;You have 3 listings...&quot;Why Traditional Assertions FailConsider this test case: ‚ÄúAsk the agent for a list of properties and verify it returns them correctly.‚ÄùWith traditional testing, you might write:def test_get_listings(): response = agent.invoke(&quot;What are my listings?&quot;) assert response == &quot;You have 3 listings: Beach House, Mountain Cabin, City Apartment&quot;This test will fail immediately. The agent might respond with: ‚ÄúI found 3 properties in your portfolio: Beach House, Mountain Cabin, and City Apartment.‚Äù ‚ÄúYou have three listings:\\n1. Beach House\\n2. Mountain Cabin\\n3. City Apartment‚Äù ‚ÄúHere are your 3 listings: Beach House, City Apartment, Mountain Cabin‚Äù (different order)All three responses are correct. They contain the same information. But string matching fails because LLMs are non-deterministic by design. Even with temperature=0, subtle variations in tokenization, context, and model state can produce different outputs.You could try fuzzy matching or regex, but this quickly becomes unmaintainable. How do you write a regex that matches ‚Äú3‚Äù, ‚Äúthree‚Äù, and ‚Äú3 properties‚Äù while rejecting ‚Äú2 listings‚Äù?The solution: let an LLM do the evaluation.The LLM-as-Judge PatternThe core insight is simple: if an LLM can generate natural language, another LLM can evaluate it. We use a separate, more capable model as a ‚Äújudge‚Äù to score the agent‚Äôs responses against specific criteria.Here‚Äôs the architecture:flowchart LR subgraph Trigger[Trigger] CLI[CLI] Web[Web App] end subgraph Framework[Test Framework] Executor[Test Executor] DB[(PostgreSQL)] end subgraph Agent[Agent Under Test] RA[ReactAgent] LLM1[GPT-5 Mini] MCP[MCP Server] end subgraph Eval[Evaluation] Judge[LLM Judge] LLM2[Claude Opus 4.5] end CLI --&amp;gt; Executor Web --&amp;gt; Executor Executor &amp;lt;--&amp;gt; DB Executor --&amp;gt; RA RA &amp;lt;--&amp;gt; LLM1 RA &amp;lt;--&amp;gt; MCP Executor --&amp;gt; Judge Judge &amp;lt;--&amp;gt; LLM2The judge receives three inputs: The original user prompt - what was asked The agent‚Äôs response - what the agent said Evaluation criteria - what constitutes a good responseIt returns: A score between 0.0 and 1.0 Reasoning explaining the scoreHere‚Äôs a simplified implementation:JUDGE_SYSTEM_PROMPT = &quot;&quot;&quot;You are an expert evaluator of AI assistant responses. Your task is to score responses based on specific criteria provided by the user.You must respond with ONLY a JSON object in this exact format:{&quot;score&quot;: 0.85, &quot;reasoning&quot;: &quot;Brief explanation of the score&quot;}The score must be a float between 0.0 and 1.0:- 1.0 = Perfect response that fully meets all criteria- 0.8+ = Good response that meets most criteria with minor issues- 0.6-0.8 = Acceptable response with some missing elements- 0.4-0.6 = Partial response with significant gaps- Below 0.4 = Poor response that fails to meet criteriaBe objective and consistent in your evaluation.&quot;&quot;&quot;async def judge_response( response_content: str, prompt: str, judge_prompt: str, threshold: float, model: str = &quot;claude-opus-4-5&quot;,) -&amp;gt; JudgeResult: evaluation_prompt = f&quot;&quot;&quot;Evaluate the following AI assistant response.&amp;lt;user_prompt&amp;gt;{prompt}&amp;lt;/user_prompt&amp;gt;&amp;lt;ai_response&amp;gt;{response_content}&amp;lt;/ai_response&amp;gt;&amp;lt;evaluation_criteria&amp;gt;{judge_prompt}&amp;lt;/evaluation_criteria&amp;gt;&quot;&quot;&quot; client = anthropic.AsyncAnthropic() message = await client.messages.create( model=model, max_tokens=500, system=JUDGE_SYSTEM_PROMPT, messages=[{&quot;role&quot;: &quot;user&quot;, &quot;content&quot;: evaluation_prompt}], ) result = json.loads(message.content[0].text) passed = result[&quot;score&quot;] &amp;gt;= threshold return JudgeResult( passed=passed, score=result[&quot;score&quot;], threshold=threshold, reasoning=result[&quot;reasoning&quot;], )The Model Hierarchy StrategyA key design decision: the judge should be more capable than the agent being tested.Our agent uses GPT-5 Mini for production, optimizing for cost and latency. But for evaluation, we use Claude Opus, a more capable model with stronger reasoning abilities.This works because: The judge doesn‚Äôt need domain knowledge - it only evaluates whether criteria are met Evaluation is simpler than generation - it‚Äôs easier to judge quality than to produce it Cost is acceptable for testing - you run tests less frequently than production queriesConfiguration example:config = { &quot;agent&quot;: { &quot;model&quot;: &quot;openai:gpt-5-mini&quot;, }, &quot;judge&quot;: { &quot;model&quot;: &quot;anthropic:claude-opus-4-5&quot;, &quot;default_threshold&quot;: 0.7, },}The Scoring Rubric ApproachThe judge uses a consistent scoring rubric: Score Meaning 1.0 Perfect response that fully meets all criteria 0.8+ Good response with minor issues 0.6-0.8 Acceptable with some missing elements 0.4-0.6 Partial response with significant gaps &amp;lt; 0.4 Poor response that fails to meet criteria Each test case defines its own evaluation criteria via a judge_prompt. This allows you to be specific about what matters for each scenario:test_case = { &quot;name&quot;: &quot;get_listing_details&quot;, &quot;prompt&quot;: &quot;What are the details of my Beach House listing?&quot;, &quot;expected_tools&quot;: [&quot;get_listing_details&quot;], &quot;judge_prompt&quot;: &quot;&quot;&quot;The response should contain: - The listing name (Beach House) - The property address - Current pricing information - Occupancy rate or availability status The information should be formatted clearly and be easy to read.&quot;&quot;&quot;, &quot;judge_threshold&quot;: 0.7,}The threshold is configurable per test. Critical paths might require 0.9+, while exploratory features might accept 0.6.Multi-Round and Agent Revival TestingReal conversations aren‚Äôt single-turn. Users ask follow-up questions, refer to previous context, and expect the agent to remember what was discussed.Our framework supports multi-round testing:test_case = { &quot;name&quot;: &quot;multi_round_conversation&quot;, &quot;rounds&quot;: [ { &quot;prompt&quot;: &quot;What are my listings?&quot;, &quot;judge_prompt&quot;: &quot;Should list all properties&quot;, }, { &quot;prompt&quot;: &quot;What&#39;s the occupancy for the first one?&quot;, &quot;judge_prompt&quot;: &quot;Should reference the first listing from previous response and show occupancy data&quot;, }, ], &quot;test_agent_revival&quot;: True,}The test_agent_revival flag is particularly interesting. It tests whether the agent can: Complete round 1 Close the session entirely Reopen with the same agent ID Continue the conversation with context preservedThis validates that your checkpointing and state persistence work correctly, catching bugs that would only appear when users return to conversations after server restarts.Complementary ValidatorsThe LLM judge is powerful but not the only validator. We combine it with deterministic checks:Tool ValidatorVerify that expected MCP tools were called:def validate_tools(response, expected_tools): called_tools = extract_tool_calls(response) missing = [t for t in expected_tools if t not in called_tools] return ToolValidationResult( passed=len(missing) == 0, expected_tools=expected_tools, called_tools=called_tools, missing_tools=missing, )This is deterministic. If the test expects get_listings to be called and it wasn‚Äôt, that‚Äôs a clear failure regardless of what the response says.Database ValidatorVerify that conversations and messages are persisted correctly:def validate_database(agent_id, verify_conversation=True, verify_messages=True): if verify_conversation: conversation = get_conversation_by_id(agent_id) if not conversation: return DatabaseValidationResult(passed=False, message=&quot;Conversation not found&quot;) if verify_messages: messages = get_messages_by_thread(agent_id) if len(messages) &amp;lt; 2: return DatabaseValidationResult(passed=False, message=&quot;Expected at least 2 messages&quot;) return DatabaseValidationResult(passed=True)The Follow-up Detection PatternHere‚Äôs an edge case that caused many false failures: sometimes the agent asks for clarification instead of answering.User: ‚ÄúWhat‚Äôs the occupancy?‚ÄùAgent: ‚ÄúI‚Äôd be happy to help! Which listing would you like to see the occupancy for?‚ÄùThis is valid agent behavior, but it fails a test expecting occupancy data. The solution: use another LLM to detect clarification requests and auto-respond.async def check_needs_followup(ai_response: str, original_prompt: str) -&amp;gt; tuple[bool, str]: check_prompt = f&quot;&quot;&quot;Analyze this AI response to determine if it&#39;s asking for clarification.&amp;lt;original_user_request&amp;gt;{original_prompt}&amp;lt;/original_user_request&amp;gt;&amp;lt;ai_response&amp;gt;{ai_response}&amp;lt;/ai_response&amp;gt;If the AI is asking for clarification, respond with:{{&quot;needs_followup&quot;: true, &quot;followup_response&quot;: &quot;A reasonable response providing the needed clarification&quot;}}If the AI provided a substantive answer, respond with:{{&quot;needs_followup&quot;: false, &quot;followup_response&quot;: &quot;&quot;}}&quot;&quot;&quot; client = anthropic.AsyncAnthropic() message = await client.messages.create( model=&quot;claude-sonnet-4-20250514&quot;, max_tokens=300, messages=[{&quot;role&quot;: &quot;user&quot;, &quot;content&quot;: check_prompt}], ) result = json.loads(message.content[0].text) return result[&quot;needs_followup&quot;], result[&quot;followup_response&quot;]The executor uses this to automatically continue conversations up to a maximum number of follow-ups, ensuring tests evaluate the final answer rather than intermediate clarifications.Democratizing Test Creation: The Web UIOne final insight: testing shouldn‚Äôt be engineers-only.The web UI shows test results with judge scores and reasoning for each roundProduct managers often understand user intent better than engineers. They know what users actually ask and what good answers look like. But if tests live in code, PMs can‚Äôt contribute.Our solution: store test cases in PostgreSQL and build a web UI for creating and managing them.Why a database instead of config files: Non-engineers can participate without code changes Test history and results are preserved Easy to build dashboards and analytics Supports the web UI and CLI equallyDual interfaces: CLI for engineers and CI pipelines Web App for everyone elseBenefits: PMs can define test scenarios based on real user questions Faster iteration on test coverage No pull requests required to add new tests Visibility into test results for the whole teamThe web UI lets anyone create a test case by filling in: A name and description The user prompt Expected tools (optional) Judge evaluation criteria Pass/fail thresholdResults are stored and displayed in a dashboard, making it easy to track agent quality over time.ConclusionTesting AI agents requires a fundamental shift in approach. Traditional assertions fail because LLM outputs are non-deterministic by design. The LLM-as-judge pattern solves this by using a more capable model to evaluate responses against semantic criteria rather than exact matches.Key takeaways: Use a superior model as judge - Claude Opus evaluating GPT-5 Mini responses works well because evaluation is simpler than generation. Combine LLM evaluation with deterministic checks - Tool validation and database verification catch issues that semantic evaluation might miss. Test multi-round conversations - Real users have follow-up questions. Test that context persists correctly. Handle clarification loops - Agents asking for clarification is valid behavior. Detect it and auto-respond to get to the actual answer. Democratize test creation - Store tests in a database with a web UI so non-engineers can contribute. They often understand user intent better than we do. The result is a testing framework that catches real regressions while tolerating the natural variation in LLM outputs. Our agents are more reliable, and our whole team can contribute to quality." }, { "title": "Mocking requests to third party services", "url": "/posts/mocking-requests-to-third-party-services/", "categories": "", "tags": "", "date": "2022-08-09 19:00:00 -0500", "snippet": "Third party services are frequently used to extend the functionality of our product/service, especially in cases where what is needed grows beyond the limits of our industry or purpose. Once an integration is set up and running, naturally this must be included in our automation tests, right? (right?)It is a widely accepted practice to always mock requests made to third party services while testing, integration tests included. The reasoning behind this lies in 2 main areas: We don‚Äôt control the response we get. This may lead to flaky tests, our testing suite failing because of service unavailability etc. The negative impact is even greater when a CI/CD process is used to deploy to our production and staging environments. Not to mention hotfixes‚Ä¶ Tests may take a while, since a connection is in the middle of it. If our suite consists of hundreds or thousands of tests, this may add up to a horrible amount of time to complete the suite.So what are the best ways to mock those requests in our automated tests?Sample caseLet‚Äôs say that in our product, we deal with international customers who pay in different currencies and we want to convert everything to let‚Äôs say USD. To do so we can use the OpenExchange API. Here is a sample code to implement this:import requestsclass ExchangeRatesClient: def __init__(self): self.base_url = &quot;https://open.er-api.com/v6/latest/&quot; def get_exchange_rates(self, currency_code): url = self.base_url + currency_code response = requests.get(url) response.raise_for_status() return response.json()Good ol‚Äô monkeypatchMonkeypatch is a fixture from pytest which allows us to easily patch attributes. In this case the get() function of requests.Example:class MockResponse: def __init__(self, json_data, status_code): self.json_data = json_data self.status_code = status_code def json(self) return self.json def raise_for_status(self) returnclass TestExchangeRatesClient: def test_get_exchange_rates(self, monkeypatch): #assign exchange_rates_client = ExchangeRatesClient() expected_value = { &quot;result&quot;: &quot;success&quot;, &quot;base_code&quot;: &quot;USD&quot;, &quot;rates&quot;: { &quot;USD&quot;: 1, &quot;CAD&quot;: 1.26, &quot;CHF&quot;: 0.941, &quot;EUR&quot;: 0.923, &quot;GBP&quot;: 0.765, &quot;JPY&quot;: 125.9, }, } mock_requests = Mock( return_value = MockResponse(json_data = expected_value, status_code = 200) ) monkeypatch.setattr(requests, &quot;get&quot;, mock_requests) #act result = exchange_rates_client.get_exchange_rates(currency_code=&quot;USD&quot;) #assert assert result == expected_valueHow it works:When requesting data, we get a Response object so we have to mock its behaviour. This is what the class MockResponse is doing. This class may change depending on how we use the response object. Eg. we may need to add a text attribute, or add logic to the raise_for_error() to mock its behaviour and raise an HTTPError in case of an invalid status_code (4xx-5xx)PROS: Doesn‚Äôt depend on 3rd party libraries Flexibility on implementing exotic cases when mockingCONS: In real life examples, the response object can be very complex (headers, huge reponses, cookies etc) which can make the MockResponse class a hairy mess Monkeypatching has a global effect. If there were more requests we had to use side_effect and things could break. If instead of requests.get we use requests.Session().get(), then monkeypatching goes bust.Requests-mockFrom the solution above we see that there is a lot of logic inside the MockResponse which isn‚Äôt always pleasant and we may introduce more bugs. And it feels a little too custom of a solution for such a generic problem to solve. So yeah‚Ä¶ you guessed it. There is a library for that. One of the mostly used libraries in the python world is requests-mockExampledef test_get_exchange_rates_with_requests_mock(self): # assign exchange_rates_client = ExchangeRatesClient() expected_value = { &quot;result&quot;: &quot;success&quot;, &quot;base_code&quot;: &quot;USD&quot;, &quot;rates&quot;: { &quot;USD&quot;: 1, &quot;CAD&quot;: 1.26, &quot;CHF&quot;: 0.941, &quot;EUR&quot;: 0.923, &quot;GBP&quot;: 0.765, &quot;JPY&quot;: 125.9, }, } # act with requests_mock.Mocker() as m: m.get(&quot;https://open.er-api.com/v6/latest/USD&quot;, json=expected_value) result = exchange_rates_client.get_exchange_rates(&quot;USD&quot;) assert result == expected_valueHow it worksWithin the context manager, every request is being mocked by declaring it using the method and url. Regex can be used for more dynamic matching. If a request is not matched, then requests_mock will throw an exception and the test will fail, thus preventing the call to an external endpoint. Here, there are arguments as well to specify status_code, json, text etc depending on the desired response.PROS Easy to use Flexibility Less code Mock response objects have the desired behaviour by defaultCONS You need to explicitly take care of every request in terms of expected values Hand written expected valuesVCR.pyStorming from Ruby‚Äôs VCR library, VCR.py brings all the 80s nostalgia that we secretly crave for is a neat and powerful library that takes away most of the pain. What it does is that it records every http request done inside a test and ‚Äúwrites‚Äù them in a (by default .yaml) file. This file is then reused every time a request needs to be mocked. If there are more than one requests, then it records them all and are exposed in the cassette.responses list.Exampledef test_get_exchange_rates_with_vcr(self): # assign exchange_rates_client = ExchangeRatesClient() # act with vcr.use_cassette( &quot;exchange_rates.yaml&quot;, serializer=&quot;yaml&quot;, decode_compressed_response=True ) as cassette: result = exchange_rates_client.get_exchange_rates(&quot;USD&quot;) # assert assert result == json.loads(cassette.responses[0][&quot;body&quot;][&quot;string&quot;])How it worksThis library is very flexible and can be configured in many different ways.Decorator vs context managerFirst it can be used as a decorator to mock all requests within a test, or as a context manager to mock requests in a specific area inside the test, allowing to have different VCR objects per request with different configurations. The vcr.use_cassette() can be used to specify the file to write and/or the path of that file. Ex. with vcr.use_cassette(‚Äúpath/to/folder/exchange_rates.yaml‚Äù)Record modesonce (default)When a request is about to happen, it checks if the file exists. If not, it makes an actual request, and writes it into the file. IF the file exists already, it uses that file instead of the actual request. If the file exists but the request is different than the one that it is recorded in, then it raises an error. This is the most used record mode. Usually we make a real request (with api_keys etc) and then we remove the sensitive code (we can also filter headers, query_parameters as described here)If something goes wrong, then we can simply delete the .yaml file and run the test again.noneNever make a real http request. It is mostly used when we want to be absolutely sure that an http endpoint is never called because that endpoint is sensitive.allAlways make an http request. This mode is not for mocking but when we want to override an existing cassette without deleting the file or when we want to simply log requests (rare)new_episodesLike the once mode, but in case there is a file with a similar request, it overrides it instead of raising an exceptionSample ConfigurationA very common blocker when first using this library is to create a generic configuration that all subsequent tests can easily use. Here is a personal favourite: a double wrapped func. Config file:# here more params can be addeddef vcr_cassette(cassette_path): def inner(func): @wraps(func) def conf_vcr(*args, **kwargs): # default config vcr = VCR( record_mode=record_mode.RecordMode.ONCE, decode_compressed_response=True, serializer=&#39;yaml&#39;, cassette_library_dir=os.path.join( os.path.dirname(os.path.realpath(__file__)) + &#39;/cassettes/&#39; ), ) with vcr.use_cassette(cassette_path): return func(*args, **kwargs) return conf_vcr return innerUsage:@vcr_cassette(&quot;exchange_rates.yaml&quot;)def test_get_exchange_rates_with_vcr(self): # assign exchange_rates_client = ExchangeRatesClient() # act result = exchange_rates_client.get_exchange_rates(&quot;USD&quot;) # assert assert result == json.loads(cassette.responses[0][&quot;body&quot;][&quot;string&quot;])PROS Easy to use Flexibility Stores real responses and is easy to record the complex ones that would be a nightmare to mock them manually Ability to view those responses Ability to hide/not record sensitive information like tokens/api keys etc Easily record multiple calls within a test, which in normal cases would be hard to mock.CONS Every time there is a change in a request, that means that a response needs to be generated. But what happens if that response was time sensitive and changes in time? (Trick is to - manually tweak the request). A live request is needed if the record mode is set to ONCE. Sometimes this is impossible. Configuration can be daunting at firstConclusionSo there you have it. When dealing with third parties there is a range of strategies that can fit to a variety of cases. Here at Beyond we prefer the feature rich and production-ready solution of the VCR library, since we are doing a heavy use of external requests. The other solutions proposed can also be an ideal approach, especially for simpler cases." }, { "title": "Tools to backfill large PostgreSQL tables (part 1)", "url": "/posts/tools-to-backfill-large-postgresql-tables-pt1/", "categories": "", "tags": "", "date": "2022-05-31 19:00:00 -0500", "snippet": "At Beyond we are mainly using PostgreSQL databases for our services, consisting of over a hundred tables. With some of the largest ones we‚Äôve already had a few close calls as they were reaching the maximum supported 32TB table size - but that‚Äôs a story for another day. Backfilling tables that large is not a trivial task, assuming we want to perform it safely, and within a reasonable timeframe. In this post, we are going to outline a few methods and tools similar to what we have successfully used to make this process scalable, and a little less painful.What‚Äôs backfilling and why can it be a problem?Backfilling simply means modifying or adding new data to existing database records, i.e. running UPDATE statements, but with an emphasis on performing it on all or most of the records in a table.When would it be needed? We want to modify a column‚Äôs value to fix faulty data We have to add a new field, i.e. insert a new column to the table, with either a default value or with data computed by arbitrary business logic.In any case, the main issue is that if we want to perform a backfill on a seriously large table, it just simply takes a very long time. And by that I mean if we just naively issue an UPDATE table SET field=value WHERE id=n for each and every row, it will finish in weeks, months, or even years, depending on the use case‚Äôs context. In a specific case, we had to update a 22TB table with hundreds of billions of rows where the estimated time to completion was over 2 years.With tables of this size, just creating a copy of the whole table and doing the update on it to avoid certain problems is not always a viable option, so we‚Äôll focus on the use case where we really want to update the live table.The first challenge is to complete this process as quickly as possible, naturally. The second one is to accept that it won‚Äôt be quick enough, and come up with tools that make this task more controlled and manageable, while making sure our application can still operate on the table without major hiccups.What can we do?Our fictional scenario will be performed on a table named users. We‚Äôll act like we have billions of users in our system. We want to backfill this table, inserting values into our newly created reversed_last_name column where we want to store our users‚Äô last name, but, wait for it: reversed. And we want to do this because of, well, reasons. We will also pretend this is something that needs some calculations beforehand in a script and we couldn‚Äôt possibly do the update with just a snappy SQL expression (you know, like REVERSE()).Let‚Äôs perform the update in batchesUpdating rows one by one puts a massive overhead on the process, and is the primary culprit for slowness. The solution seems simple enough: use as few individual UPDATE statements as possible. To do this, we want to write our data migration script so that it will output SQL which uses PostgreSQL‚Äôs CASE expression:UPDATE users SET reversed_last_name = ( CASE WHEN id = 1 THEN &#39;htulB&#39; WHEN id = 2 THEN &#39;nasemraP&#39; WHEN id = 3 THEN &#39;walboL&#39; [...] ELSE last_name END)WHERE id &amp;gt;= 1 AND id &amp;lt;= 500As you can see from the WHERE clause, we want to build update statements only for a subset of data. We might be tempted to build one huge UPDATE on all the rows, but there are a couple of issues with that. You‚Äôll find that PostgreSQL puts an entire table lock on this huge table until the statement execution is completed, which unfortunately still won‚Äôt happen fast enough. Besides our application being denied write access to any rows, it would very likely cause a myriad of other issues. What works best is finding an equilibrium, and executing the updates in batches, balancing the number of rows affected in one batch, being aware that they will be locked while being updated, but processed and released from the lock in a reasonable amount of time.To help us compose these batch operations, we‚Äôll create a simple iterator class that will receive a database connection, the name of the table we want to backfill, the desired batch size, and an optional start_batch indicator that will come in handy later, when we implement a pause/resume feature and parallel processing. The class retrieves the max id of the table, and will yield us id boundaries according to the batch size, with a begin_id and an end_id which will help us build our select queries and batched update statements.This is a simplified example and it assumes we have integer primary keys called id and we have a data set starting from id=1.import mathclass TableBatchIterator: def __init__(self, conn, table, batch_size, start_batch=1): self.batch_size = batch_size self.start_batch = start_batch self.max_id = self.table_max_id(conn, table) self.total_batches = math.ceil(self.max_id / batch_size) def __iter__(self): self.current_batch = self.start_batch return self def __next__(self): if self.current_batch &amp;gt; self.total_batches: raise StopIteration begin_id = (self.current_batch - 1) * self.batch_size + 1 end_id = min(self.current_batch * self.batch_size, self.max_id) self.current_batch += 1 return (begin_id, end_id) @staticmethod def table_max_id(conn, table): with self.conn.cursor() as cursor: cursor.execute(f&#39;SELECT max(id) FROM {table};&#39;) row = cursor.fetchone() return row[0]To build the update statements shown above, we‚Äôll also need something that takes the begin and end id values, the field name, and the new value. We‚Äôll use a simple to use class for this.The BatchUpdateBuilder class exposes two public methods: update(), which will be responsible for accumulating all the new values we want to persist for a given id and column name, and build_sql(), which will return the corresponding UPDATE SQL statement string with the CASE/WHEN expressions built from the values stored by update(). It will also append a WHERE clause to the statement to narrow down the scope to rows concerned in the given batch.import jsonclass BatchUpdateBuilder: def __init__(self, table, begin_id, end_id): self.table = table self.begin_id = begin_id self.end_id = end_id self.updates = {} def update(self, _id, field, new_value): field_updates = self.updates.setdefault(field, {}) field_updates[_id] = new_value def build_sql(self, begin_id, end_id): if not self.updates: return None sql = ( f&#39;UPDATE {self.table}\\\\n&#39; f&#39;SET\\\\n{self._build_cases()}\\\\n&#39; f&#39;WHERE {self._build_where(begin_id, end_id)}&#39; ) return sql def _build_cases(self): cases = [] for field, field_updates in self.updates.items(): cases.append(self._build_field_case(field, field_updates)) return &#39;,\\\\n&#39;.join(cases) def _build_field_case(self, field, field_updates): cases = [] for _id, new_value in field_updates.items(): cases.append(f&#39;WHEN id = {_id} THEN {self._to_sql_value(new_value)}&#39;) cases = &#39;\\\\n &#39;.join(cases) return ( f&#39; {field} = (\\\\n&#39; &#39; CASE\\\\n&#39; f&#39; {cases}\\\\n&#39; f&#39; ELSE {field}\\\\n&#39; &#39; END\\\\n&#39; &#39; )&#39; ) def _build_where(self): return f&#39;id &amp;gt;= {self.begin_id} AND id &amp;lt;= {self.end_id}&#39; @staticmethod def _to_sql_value(value): if type(value) in (dict, list): return f&#39;\\\\&#39;{json.dumps(value)}\\\\&#39;::jsonb&#39; elif type(value) is str: return f&#39;\\\\&#39;{value}\\\\&#39;&#39; elif value is None: return &#39;null&#39; else: return str(value)With these in place, we can do something like this, demonstrating with a Django example:from django.db import connectionfrom . import BatchUpdateBuilder, TableBatchIteratorBATCH_SIZE = 500TABLE_NAME = &#39;users&#39;table_batch_iterator = TableBatchIterator( conn=connection, table=TABLE_NAME, batch_size=BATCH_SIZE)cursor = connection.cursor()for boundaries in table_batch_iterator: begin_id, end_id = boundaries batch_update_builder = BatchUpdateBuilder(TABLE_NAME, begin_id, end_id) users = User.objects.select_for_update().filter(id__gte=begin_id, id__lte=end_id) for user in users: reversed_last_name = user.last_name[::-1] batch_update_builder.update(user.id, &#39;reversed_last_name&#39;, reversed_last_name) sql = batch_update_builder.build_sql() cursor.execute(sql)cursor.close()Pretty straightforward. We create an instance of the TableBatchIterator instrumented to perform the updates in batches of 500 (at most, actual batch size may vary if rows with certain id values are missing) on our users table, then iterate through it to build and execute a bunch of UPDATE statements generated by the BatchUpdateBuilder.Moving forwardIn the second part of this blog post we‚Äôll be going further, adding a progress calculator to help us monitor the status of the process, and as a last step we will create a lightweight queue system that makes it possible to run the task in parallel processes while also allowing us to pause and resume the process on demand." }, { "title": "Empathy and Compassion in Communication (part 2)", "url": "/posts/empathy-and-compassion-in-communication-part-2/", "categories": "", "tags": "", "date": "2022-04-11 23:00:00 -0500", "snippet": "Effective engineers are strong problem solvers. Great engineers have also become inspiring leaders and master communicators. Whether you want to take your career to the next level, or simply aspire to round-up your skillset, leveling up your self-awareness and communication skills will open many doors on your personal and professional path. In a spirit of sharing, here are four lessons I have learned.This is a continuation of part 13. Resort to your problem-solving abilitiesTL;DRAnalyze problems in a systematic, pragmatic way, and don‚Äôt stop thereAssess whether you want to tackle the situation or notAsk yourself ‚ÄúWhat is it that I can do about it?‚ÄùUse a language of gaps and possibilities rather than one of problemsPropose multiple solutions, as diverse as possibleHonestly list the pros/cons staying away from your own biasesEstablish a plan to bridge the gap and a way to measure progressFollow up regularlyAvoid resorting to problem solving when you engage with someone in an emotionally challenging place, unless they request it.As engineers and scientists, we have an innate curiosity and desire to figure out how systems work. By understanding how something works, we can come up with hypotheses on how certain changes affect its behavior, and subsequently empirically verify our predictions. Business and communication are no different and can be looked at under this same lens.Identifying a problem however, is just the first step toward speaking it out. As a young engineer, I often found myself stopping at that step and just communicating the problem once identified. That was a fundamental mistake. Despite noble intentions, we will often be perceived as problem makers, not problem solvers.Once we identify a problem (the ‚ÄúWhat‚Äù), the first thing we need to address is whether or not there is enough at stake to justify the cost of beginning to explore solutions and eventually solving it (the ‚ÄúIf‚Äù). We subconsciously do this all the time, when we screen out information to focus on what matters. The same can be applied to problem solving. Not every single problem needs to be solved. Our bandwidth and willpower are limited, and we ought to prioritize and focus on what matters from a business, personal or relationships standpoint.We can start to reflect and prepare our communication. One of the most empowering question we can ask ourselves anytime we encounter a challenge is: ‚ÄúWhat is it that I can do about it?‚Äù. Put emphasis on the ‚ÄúI‚Äù and point both index fingers at yourself while asking this question. Sometimes it feels like things are out of our hands. But in truth, there is always something we can do about it. We always have a choice to talk and act in a candid fashion, or not to. Taking ownership of our part is an efficient way to start a positive dynamic with compassionate intentions, and will set us up for success when we engage in communication.If we decide to move forward, we then need to clearly articulate the ‚ÄúWhat‚Äù we identified in the previous step. Most often, a good approach is to describe the issue as a gap and quantifying it whenever possible. Expressing a gap is simply saying ‚ÄúHere we are. We want to get there. What can we do about it?‚Äù Speaking in terms of gaps is a language of possibilities and opportunities instead of a language of problems, concerns, blame, and exhortation. Here is an example that illustrates the contrast between the two approaches:Problem language: ‚ÄúI have some bad news. We are tracking behind our sales target.‚ÄùPossibilities language: ‚ÄúIt is January 31st and we are currently at $200K in sales for Q1. Our goal is to reach $900K by the end of Q1. So, we are currently tracking $100K behind. Let‚Äôs talk, what can we can do to bridge that gap?‚ÄùPossibility lies in making a difference and creating value from a situation, without denying that certain issues exist. ¬π -Benjamin ZanderThe next step is challenging ourselves to express as many potential solutions as we can think of. It is important to keep our mind open, as we always tend to fall into our biases and convincing ourselves that ‚ÄúThis is the only real way.‚Äù At this point, we really help resolve issues and become problem solvers. An efficient way to keep each other in check when exploring and comparing multiple options, is to pragmatically list the pros and cons for each of them.Once a plan is chosen to address the gap, we can figure out ways to measure progress as objectively as possible. Timelines and Gantt charts are often applicable here, and we can also decide to measure progress based on value output KPIs, which take into account key parts of the gap we are bridging. The trick here is to find the right balance where what is measure is not too generic and abstract and not too specific either.Finally, make a point to follow up regularly. Failing to do so is failing to promote healthy accountability, and somewhat nullifies all the work that was done to this point. We can share status updates, use information radiators such as digital and physical boards and foster a culture transparency, accountability and predictability.Problem solving and gap bridging are great tools to have in our bag, but they are not adapted to every situation. In fact they can be destructive and counterproductive when someone is emotionally struggling. In such instances we need to resort to simply expressing empathy and practicing compassion.4. Show empathy and practice compassionTL;DREmpathy and compassion are crucial at all levels of communication, especially in conflict resolution and emotional supportRefrain from using problem solving (3.)Put your opinions aside and listen (1.)Validate his/her perspectiveIf you can genuinely relate, express your feelingsAsk what he/she would do, and refrain from debatingSometime just being there in silence is enough, and sometime being compassionate is respecting the need of others to be aloneSometimes we find ourselves communicating with someone who is struggling emotionally, perhaps experiencing a difficult problem at work or going through a challenging time in their personal life. In these situations, avoid resorting to problem solving. When someone is overwhelmed by their feelings, we want to validate their feelings and support them emotionally, thereby giving them something to relate to and to hold onto.It is therefore important to refrain from diminishing how someone feels. Sugar-coating or trying to minimize the impact of the situation at stake can sound like this:‚ÄúThis is no big deal, don‚Äôt worry you will be alright‚ÄùHere the message is:‚ÄúI can‚Äôt relate to you. I have no idea what you are going through and I can‚Äôt feel your pain.‚ÄùSimilarly, we want to be very wary of patronizing:‚ÄúIf you think what happened to you is bad, let me tell you what I went through last year‚Ä¶‚ÄùWe should also refrain from resorting to our engineering approach of providing solutions when someone is frustrated, upset, sad, angry,‚Ä¶ Instead, we can come down from our head to our heart and our senses, and give the gift of our support and our presence. Most often, that just mean sitting next to them and saying:‚ÄúThis sucks. I know how much of a big deal this is for you. I saw you do everything you could.‚ÄùShow up for people in pain and don‚Äôt look away. If we want people to bring their whole selves including their unarmored, whole hearts ‚Äîso that we can innovate, solve problems, and serve people‚Äî we have to be vigilant about creating a culture in which people feel safe, seen, heard, and respected.¬≤-Br√©n√© BrownWe can also resort to simple construction to show empathy, even when we are facing a conflictive situation¬≥:‚ÄúAre you feeling &amp;lt;guess their feeling&amp;gt;, because you need &amp;lt;guess their need&amp;gt;?‚ÄùThis can help us connect in a kind and compassionate way. It doesn‚Äôt matter whether we accurately identify the feeling or the need behind it, as long as we show a genuine intent to connect on that level.Dao ÈÅìImproving our communication and EQ is not as easy as reading a blog post. This territory seems completely limitless to me, made of a myriad of skills of all sizes from learning to say ‚ÄúThank you!‚Äù to public speaking. Each one complements the others and, as we practice them, make us well rounded communicators. And so, if you wish to become a speaker, a leader or simply a better spouse, parent or colleague, prepare yourself for a lifelong adventure of learning and enjoy the ride! There is no end goal here, it is about the journey toward the ideal. Only by understanding that we can never reach it, can we get closer to it.Thank you for taking the time to read this article. I am grateful for your time and consideration. I will cherish the gift of feedback, and would appreciate if you could tell me in the comments: The one concept that most resonated with you Any suggestions you may have to improve the content or the deliveryThank you! üôè1: Benjamin Zanders, Choosing Your World, Google ZeitGeist, 2011 ‚Ü©2: Br√©n√© Brown, Dare to Lead ‚Ü©3: Marshall Rosenberg, Nonviolent Communication: A Language of Life ‚Ü©" }, { "title": "Empathy and Compassion in Communication (part 1)", "url": "/posts/empathy-and-compassion-in-communication-part-1/", "categories": "", "tags": "", "date": "2022-04-10 23:00:00 -0500", "snippet": "Effective engineers are strong problem solvers. Great engineers have also become inspiring leaders and master communicators. Whether you want to take your career to the next level, or simply aspire to round-up your skillset, leveling up your self-awareness and communication skills will open many doors on your personal and professional path. In a spirit of sharing, here are four lessons I have learned.Humans and computersThe story began when I was nine years old. Personal Computers were beginning to make their way into schools and libraries. I was privileged and fortunate enough to have one at home, and very lucky to find a friend who taught me the essentials of hardware and software. In those days you needed to know both. You would program the chip directly, there was no Garbage Collection and, in this case, a fancy 512 KB of memory to play with. Computer programming was a different animal altogether, and quite fun to me. It felt like a game, and still does to this day. And so, I followed a somewhat established path of math and science education to eventually land a job at playing my favorite game. Mission complete? Well, not quite.As time went by, I eventually realized that my need for connection with others wasn‚Äôt quite fulfilled through my studies or my work. It didn‚Äôt feel like I was helping and connecting with others in a direct and meaningful way. I was intrigued and curious about other business functions, especially product and marketing, which heavily rely on communication. ‚ÄúWhat‚Äôs so hard about it?‚Äù, I thought. I was in for quite a surprise. I could never have imagined, even in my wildest dreams, how subtle, complex and refined that new game can get.As I decide to undertake the journey and start a new business in Chile in 2002, I soon realize that the task at hand was daunting, not that of building our product, but that of building our product together. This adventure of collaboration altered the course of my journey. It crystallized into a realization that ‚ÄúCommunicating with computers is far simpler than communicating with people‚Äù, or to put it in reassuring engineering language, ‚ÄúPeople are systems of immense complexity, and spoken languages, protocols of colossal intricacy.‚ÄùSo I began learning. I listened to the advice of peers and proven communicators and carefully watched them in action. I read books on effective communication, emotional intelligence, and team leadership and practiced one skill at a time before moving on to the next one. One of the most important steps I took was reaching out to teammates whom I had failed to listen to, understand and support effectively. I asked them what I could have done better, and I listened.Learning, to me, encompasses the purpose of life. It is playful in its essence and makes living about the journey rather than the destination.Four principles I try to practice along the way1. Listen, listen, listen‚Ä¶ and watchTL;DRTry and listen with your ears, eyes and heart, not just with your headSpeak tentatively and create space for others to safely share their storiesWatch and listen to yourself as much as othersStay candid when it is hard to do so, that is when it matters the mostLeverage mirroring and other active listening techniquesExploring possibilities with teammates, friends and mentors is often a good way to start learning any skill. So I asked around.‚ÄúIn your opinion, what are the three most important skills in communication?‚ÄùNearly invariably, ‚ÄúListening‚Äù was part of the answer. David Deal, whom I consider an expert craftsman in that field, and who happens to help me write these very lines, answered that question with ‚Äú1. Listen, 2. Listen, 3. Listen.‚ÄùWhat does it mean to listen, and how can we get better at it? I believe the answer greatly depends on our degrees of extroversion, and intuition. Listening is in part creating space and safety for others to participate in the exchange of information. The more extroverted we are in relation to others, the more we should be self-aware of our propensity to occupy that space and refrain from doing so. Listening means opening our mind to the ideas of others, and broadening our perspective by leveraging diversity of thoughts.‚ÄúListen with the intent to understand, not respond.‚Äù ¬π -Stephen CoveyThe more intuitive we are, the more we tend to solve puzzles and search for information inside instead of outside. And so we listen to our own internal voice using our mind rather than to the voice of others, using our ears. If you are intuitive, which can be considered a trait for many engineers, try to tame your inside voice and make the effort to clearly listen to others. If you are ever in doubt, you can always use paraphrasing and other active listening techniques. Many experiments have proven that the exact intended meaning of a message, is nearly never fully understood. And so, we can always pause and resort to constructions such as:‚ÄúThis is what I heard you say: &amp;lt;fill in&amp;gt;. I understand what you mean is &amp;lt;extract meaning&amp;gt;. Is that right?‚ÄùFinally, being in receiving mode means listening for a message. That message is not solely composed of words. If we are in an oral conversation, intonation becomes a crucial part of the meaning. Even more importantly, body language can reveal an undercurrent that is most often unintentional. It is therefore critical to learn to look and become attuned with one‚Äôs emotion inside, and others‚Äô outside, as they tell us a raw, unfiltered story about ourselves, our teammates and about the dynamics in a meeting room.2. Stick to observations, avoid judgements and evaluationsTL;DRSlow down, recognize when you are telling yourself ‚Äúvillains or victims‚Äù storiesSearch for explanations that presuppose of positive intentQuestion yourself to grow. You are the only one that you can changeUse a pragmatic language of observations devoid of evaluationsOne of the major hurdles we create for ourselves is making suppositions that, in the blink of an eye become assumptions, increasing and confirming our biases while dictating our emotional response, and subsequently our behavior. These stories are often unconsciously designed to validate our pre-existing biases and to explain the world in a way that doesn‚Äôt require questioning ourselves, thereby preventing us from learning and growing.Our prefrontal cortex (part of the new hardware that evolution has provided us) is in a perpetual quest of understanding and predicting the world around us. It does so very fast, and generally assumes a worst case scenario. This evolution design is at the root of ‚Äúfight, flight, or freeze‚Äù and other primitive behaviors that helped our ancestors escape predators. This kind of responses, however, are counterproductive in our modern world, where teamwork and collaboration are essential to success.Validating assumptions that support the shortest and the easiest way to an explanation, prevents us from questioning ourselves. This can result in finger pointing supported by our invented villain and victim stories. It prevents us as a group from understanding a situation objectively. On the other hand, we can decide to ask ourselves questions such as ‚ÄúWhere could I be mistaken?‚Äù and ‚ÄúWhat can I do about the current situation?‚Äù‚ÄúBetween stimulus and response there is a space.In that space is our power to choose our response.In our response lies our growth and our freedom.‚Äù-UnknownAs scientists and engineers, we tend to be pragmatic, at least in the practice of our craft. However, staying away from judgements and evaluation of intentions of others and of ourselves can be tricky. Developing a pragmatic language of observations, and abstaining from judging intentions or pretending we are mind readers can help a lot.We know we are emitting a judgment when we give implicit intention to the subject in our sentence. For example:- ‚ÄúMy teenage daughter is disrespectful to adults.‚Äù- ‚ÄúJoey ridiculed me in front of our boss at this morning‚Äôs stand-up‚ÄùWe can choose to stay away from judgements and evaluation by candidly sticking to observations. Observations are often expressed as gaps between what we observe and what we expected. Here are a few examples from Marshall Rosenberg‚Äôs Nonviolent Communication¬≤:Evaluation: ‚ÄúThe boss is procrastinating around this decision.‚ÄùObservation: ‚ÄúThe boss told us she would announce the decision by last week, but we still haven‚Äôt heard.‚ÄùEvaluation: ‚ÄúYou lied to me about your grades.‚ÄùObservation: ‚ÄúI heard you say you passed all courses, but this report shows two F‚Äôs.‚ÄùNote that both these examples imply a negative moral judgement, or an assumption of negative intent on the other party.We should also be aware that construct such as ‚Äúhe/she thinks that‚Äù, are assertions that imply mind reading. It is another form of evaluation that steers us away from facts and reality. For example:- ‚ÄúMy boss thinks I don‚Äôt care about the project.‚Äù- ‚ÄúMy husband believes I don‚Äôt love him.‚ÄùNow, let‚Äôs go back to the judgment:- ‚ÄúJoey ridiculed me in front of our boss at this morning‚Äôs stand-up.‚ÄùThe first story that comes to mind might be:- ‚ÄúJoey wants to get a promotion and therefore he made fun of me at the stand-up to assert his dominance in front of our boss.‚ÄùIt is easy to spot a villain and victim story here. This is a good signal to prompt the following question:- ‚ÄúCan I think of something else that would explain Joey‚Äôs behavior without making him a villain or me a victim?‚ÄùBy doing so and pausing time between stimulus and response, we might end up with other stories such as:- ‚ÄúJoey saw our boss was uncomfortable and wanted to crack a joke. He wanted to re-establish safety in the conversation for him. He also fully expected me to be witty and come back at him to further diffuse the tension in the room. It was nothing personal.‚ÄùWe have no real way to fully assess someone else‚Äôs intentions. What we can do is choosing the stories we tell ourselves and own the space between stimulus and response.Dear reader, if you are interested in reading more, you can head to part 2.Thank you for taking the time to read this article. I am grateful for your time and consideration. I will cherish the gift of feedback, and would appreciate if you could tell me in the comments: The one concept that most resonated with you Any suggestions you may have to improve the content or the deliveryThank you! üôè1: Stephen R Covey, The 7 Habits of Highly Effective People Habit #5: Seek first to understand, then to be understood.‚Ü©2: Marshall Rosenberg, Nonviolent Communication: A Language of Life ‚Ü©" }, { "title": "Post-mortems intro", "url": "/posts/post-mortems-intro/", "categories": "", "tags": "", "date": "2022-02-01 21:33:00 -0600", "snippet": "BackgroundIn the past, one of the Beyond engineering core values was ‚ÄúWe Ship It!‚Äù More recently, we translated it to ‚ÄúShip Early.‚Äù At Beyond, the best way to ship safely is to ship often, and the best way to ship often is to ship smaller.Shipping early is a good way to get feedback, to quickly iterate and improve on code, break wrong assumptions, rearrange logic and fix unexpected (hopefully) small errors‚Ä¶We work towards perfection, but we realistically know we will never be 100% perfect, not all the time. Releases do not always follow the happy path; something unexpected may happen and then we honor the other Beyond engineering principle, ‚ÄúWe Take Ownership.‚Äù We jump into action and quickly revert or fix those errors.‚ÄúWhat went wrong, and how/what do we learn from it?‚ÄùIf we take notes during the ‚Äúfixing process,‚Äù from the time we found out something was wrong and what we have done to make it right, we can build a post-mortem!The concept of ‚Äúpost-mortem‚Äù originates from medical jargon. By definition ‚Äúit is a surgical examination of a dead body in order to find out the cause of death.‚Äù This analogy can be transcribed to the engineering field as a detailed examination of an incident after it has happened. It is a useful tool to describe major incidents for the rest of the team to have a clearer picture of what went wrong and what was done to fix it!Post-mortems are intended to help us learn from past incidents and mistakes to avoid repeating them in the future.The value of post-mortems fits perfectly with Beyond‚Äôs culture since we advocate for everyone‚Äôs continuous improvement! We embrace our mistakes and learn from them!Other companies like Google, Facebook, Amazon, and Github have also adopted this type of documentation. Here you can find a github repository with an extensive list of post-mortems from those companies.What are post-mortems and why are they valuable for Beyond?As Beyond founder and CTO David Kelso once said: ‚ÄúIt‚Äôs never fun when mistakes (‚Ä¶) happen, but what really matters is how we handle them.‚Äù At Beyond we do not point fingers, we do not blame people if they make mistakes, because in the end we all are humans and humans are very error prone!While someone is fixing a bigger issue, they are 100% focused on putting things back on the right track as soon as possible. They cannot (and should not) be wasting time and mental energy thinking about optimization or performing a deep dive on what caused the incident, we just want to get everything right very quickly! A good practice we tend to follow here, is to take notes during the problem solving because this will help to build great post-mortems that will be essential to provide an opportunity to reflect once the issue is no longer impacting users.Without a post-mortem, we may fail to recognize what we were doing right, where we could improve, and most importantly, how to avoid making the same mistakes in the future and come up with action items for a better long term solution.Still we need to emphasize that post-mortems are optional at our company. They are internal only, more targeted to engineering, but can also be really helpful for other departments like support and customer success managers to understand what happened and to help them polish their public messaging to our affected customers.‚ÄúIt‚Äôs never fun when mistakes (‚Ä¶) happen, but what really matters is how we handle them.‚Äù - David KelsoBasic Structure of a Post-mortemThese are all the areas that we explore with a post-mortem at Beyond: Overview - should be a super short 1 or 2 sentence description with quick reference to timeline, what happened and impact - consider this a ‚ÄúTLDR‚Äù section Background - If applicable, add any required background or context that is worth sharing for understanding the big picture of the problem What happened - Which servers/applications were affected? If any of the measures we took to solve the problem make it worse, we should describe them here also. Root Causes - Description of root cause for the Incident. It can be a superficial description if we are still not sure about root causes, or an in depth explanation if we are certain about the root causes. Mitigation - Basically answer the questions: ‚ÄúWhat was done to fix the issue?‚Äù and ‚ÄúHow long did it take?‚Äù Impact - This is really a very important analytical section to show the damages and the impact on the business, for example: number of users/listings affected, syncs failed, if it was a billing related issue show values breakdown, etc. We can link third party media here, like a mode report or screenshot of datadog charts, logs, etc. Responders - Name of the people who were involved with fixing the issue Timeline - From start to finish, we can detail actions attached with the time they were taken. Try to focus on the most important facts that happened; no need to have all the little details, but if you have them, you can be as specific as just putting a date or a date and a time. Good-jobs and Improvement Chances - Answer the questions: ‚ÄúWhat did we do well?‚Äù and ‚ÄúWhere‚Äôs the chance to improve?‚Äù Action Items - if a quick fix was applied, try to propose and follow up with a more structured long term optimized solution.How to write a Post-mortem?Humble advice on how to write a (fairly good) post-mortem: Once you detect that there is a problem, keep calm‚Ä¶ and start to take notes! (Of course you can just rush to make everything right fast but have small bullet points of what you have done to make things correct again, writing them down will help to build the post-mortem once the problem is fixed with peace of mind) Do not forget to populate the incident timeline with the most important changes in status and key actions taken by responders. Analyze the incident: describe superficial or in depth causes as much as you can‚Ä¶ Tell which systems/servers were affected, was it a front or backend issue? DB? etc.. For the impact part: it is nice to include charts or any other type of metric or some third-party page where the data came from. Bonus: if you can prove or illustrate that the ‚Äúrecovery‚Äù was successful. Create Jira tickets that may result from action items for a better long term solution. Write full body text, polish the messaging, and ask for a review from one of your colleagues or from your manager. Share the post-mortem. Sharing is caring! You should not feel shame - share with your teammates so we can all learn from mistakes to avoid them in the future!!!" } ]
