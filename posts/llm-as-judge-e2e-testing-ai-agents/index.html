<!DOCTYPE html><html lang="en" ><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8"><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"><meta name="day-prompt" content="days ago"><meta name="hour-prompt" content="hours ago"><meta name="minute-prompt" content="minutes ago"><meta name="justnow-prompt" content="just now"><meta name="generator" content="Jekyll v4.2.2" /><meta property="og:title" content="End-to-End Testing for AI Agents with LLM as a Judge" /><meta name="author" content="Francois Toubol" /><meta property="og:locale" content="en" /><meta name="description" content="The Testing Challenge" /><meta property="og:description" content="The Testing Challenge" /><link rel="canonical" href="https://1024inc.github.io/engineering-blog//posts/llm-as-judge-e2e-testing-ai-agents/" /><meta property="og:url" content="https://1024inc.github.io/engineering-blog//posts/llm-as-judge-e2e-testing-ai-agents/" /><meta property="og:site_name" content="Engineering @ Beyond" /><meta property="og:type" content="article" /><meta property="article:published_time" content="2025-12-25T20:00:00-06:00" /><meta name="twitter:card" content="summary" /><meta property="twitter:title" content="End-to-End Testing for AI Agents with LLM as a Judge" /><meta name="twitter:site" content="@BeyondPricing" /><meta name="twitter:creator" content="@Francois Toubol" /><meta name="google-site-verification" content="google_meta_tag_verification" /> <script type="application/ld+json"> {"@context":"https://schema.org","@type":"BlogPosting","author":{"@type":"Person","name":"Francois Toubol"},"dateModified":"2025-12-28T18:38:17-06:00","datePublished":"2025-12-25T20:00:00-06:00","description":"The Testing Challenge","headline":"End-to-End Testing for AI Agents with LLM as a Judge","mainEntityOfPage":{"@type":"WebPage","@id":"https://1024inc.github.io/engineering-blog//posts/llm-as-judge-e2e-testing-ai-agents/"},"url":"https://1024inc.github.io/engineering-blog//posts/llm-as-judge-e2e-testing-ai-agents/"}</script><title>End-to-End Testing for AI Agents with LLM as a Judge | Engineering<br/> @ Beyond</title><link rel="apple-touch-icon" sizes="180x180" href="/assets/img/favicons/apple-touch-icon.png"><link rel="icon" type="image/png" sizes="32x32" href="/assets/img/favicons/favicon-32x32.png"><link rel="icon" type="image/png" sizes="16x16" href="/assets/img/favicons/favicon-16x16.png"><link rel="manifest" href="/assets/img/favicons/site.webmanifest"><link rel="shortcut icon" href="/assets/img/favicons/favicon.ico"><meta name="apple-mobile-web-app-title" content="Engineering<br/> @ Beyond "><meta name="application-name" content="Engineering<br/> @ Beyond "><meta name="msapplication-TileColor" content="#da532c"><meta name="msapplication-config" content="/assets/img/favicons/browserconfig.xml"><meta name="theme-color" content="#ffffff"><link rel="preconnect" href="https://fonts.gstatic.com" crossorigin="anonymous"><link rel="dns-prefetch" href="https://fonts.gstatic.com"><link rel="preconnect" href="https://www.google-analytics.com" crossorigin="use-credentials"><link rel="dns-prefetch" href="https://www.google-analytics.com"><link rel="preconnect" href="https://www.googletagmanager.com" crossorigin="anonymous"><link rel="dns-prefetch" href="https://www.googletagmanager.com"><link rel="preconnect" href="https://cdn.jsdelivr.net"><link rel="dns-prefetch" href="https://cdn.jsdelivr.net"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bootstrap@4.0.0/dist/css/bootstrap.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.11.2/css/all.min.css"><link rel="stylesheet" href="/assets/css/style.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/afeld/bootstrap-toc@1.0.1/dist/bootstrap-toc.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/magnific-popup@1.1.0/dist/magnific-popup.min.css"> <script src="https://cdn.jsdelivr.net/npm/jquery@3/dist/jquery.min.js"></script><body data-spy="scroll" data-target="#toc"><div id="sidebar" class="d-flex flex-column align-items-end" lang="en"><div class="profile-wrapper text-center"><div id="avatar"> <a href="/" alt="avatar" class="mx-auto"> <img src="/assets/images/logo.png" alt="avatar" onerror="this.style.display='none'"> </a></div><div class="site-title mt-3"> <a href="/">Engineering<br/> @ Beyond </a></div><div class="site-subtitle font-italic">Let's go Beyond together!</div></div><ul class="w-100"><li class="nav-item"> <a href="/" class="nav-link"> <i class="fa-fw fas fa-home ml-xl-3 mr-xl-3 unloaded"></i> <span>HOME</span> </a><li class="nav-item"> <a href="/principles/" class="nav-link"> <i class="fa-fw fas fa-heartbeat ml-xl-3 mr-xl-3 unloaded"></i> <span>OUR PRINCIPLES</span> </a><li class="nav-item"> <a href="/obts/" class="nav-link"> <i class="fa-fw fas fa-users ml-xl-3 mr-xl-3 unloaded"></i> <span>DELIVERY TEAMS</span> </a><li class="nav-item"> <a href="/guilds/" class="nav-link"> <i class="fa-fw fas fa-lightbulb ml-xl-3 mr-xl-3 unloaded"></i> <span>GUILDS AND COMMUNITIES</span> </a><li class="nav-item"> <a href="/eng-paths/" class="nav-link"> <i class="fa-fw fas fa-users ml-xl-3 mr-xl-3 unloaded"></i> <span>ENGINEERING PATHS</span> </a><li class="nav-item"> <a href="/archives/" class="nav-link"> <i class="fa-fw fas fa-archive ml-xl-3 mr-xl-3 unloaded"></i> <span>ARCHIVES</span> </a></ul><div class="sidebar-bottom mt-auto d-flex flex-wrap justify-content-center align-items-center"> <a href="https://github.com/1024inc/" aria-label="github" class="order-3" target="_blank" rel="noopener"> <i class="fab fa-github"></i> </a> <a href="https://twitter.com/BeyondPricing" aria-label="twitter" class="order-4" target="_blank" rel="noopener"> <i class="fab fa-twitter"></i> </a> <a href=" javascript:location.href = 'mailto:' + ['dev','beyondpricing.com'].join('@')" aria-label="email" class="order-5" > <i class="fas fa-envelope"></i> </a> <a href="/feed.xml" aria-label="rss" class="order-6" > <i class="fas fa-rss"></i> </a> <span class="icon-border order-2"></span> <span id="mode-toggle-wrapper" class="order-1"> <i class="mode-toggle fas fa-adjust"></i> <script type="text/javascript"> class ModeToggle { static get MODE_KEY() { return "mode"; } static get DARK_MODE() { return "dark"; } static get LIGHT_MODE() { return "light"; } constructor() { if (this.hasMode) { if (this.isDarkMode) { if (!this.isSysDarkPrefer) { this.setDark(); } } else { if (this.isSysDarkPrefer) { this.setLight(); } } } var self = this; /* always follow the system prefers */ this.sysDarkPrefers.addListener(function() { if (self.hasMode) { if (self.isDarkMode) { if (!self.isSysDarkPrefer) { self.setDark(); } } else { if (self.isSysDarkPrefer) { self.setLight(); } } self.clearMode(); } self.updateMermaid(); }); } /* constructor() */ setDark() { $('html').attr(ModeToggle.MODE_KEY, ModeToggle.DARK_MODE); sessionStorage.setItem(ModeToggle.MODE_KEY, ModeToggle.DARK_MODE); } setLight() { $('html').attr(ModeToggle.MODE_KEY, ModeToggle.LIGHT_MODE); sessionStorage.setItem(ModeToggle.MODE_KEY, ModeToggle.LIGHT_MODE); } clearMode() { $('html').removeAttr(ModeToggle.MODE_KEY); sessionStorage.removeItem(ModeToggle.MODE_KEY); } get sysDarkPrefers() { return window.matchMedia("(prefers-color-scheme: dark)"); } get isSysDarkPrefer() { return this.sysDarkPrefers.matches; } get isDarkMode() { return this.mode == ModeToggle.DARK_MODE; } get isLightMode() { return this.mode == ModeToggle.LIGHT_MODE; } get hasMode() { return this.mode != null; } get mode() { return sessionStorage.getItem(ModeToggle.MODE_KEY); } /* get the current mode on screen */ get modeStatus() { if (this.isDarkMode || (!this.hasMode && this.isSysDarkPrefer) ) { return ModeToggle.DARK_MODE; } else { return ModeToggle.LIGHT_MODE; } } updateMermaid() { if (typeof mermaid !== "undefined") { let expectedTheme = (this.modeStatus === ModeToggle.DARK_MODE? "dark" : "default"); let config = { theme: expectedTheme }; /* re-render the SVG › <https://github.com/mermaid-js/mermaid/issues/311#issuecomment-332557344> */ $(".mermaid").each(function() { let svgCode = $(this).prev().children().html(); $(this).removeAttr("data-processed"); $(this).html(svgCode); }); mermaid.initialize(config); mermaid.init(undefined, ".mermaid"); } } flipMode() { if (this.hasMode) { if (this.isSysDarkPrefer) { if (this.isLightMode) { this.clearMode(); } else { this.setLight(); } } else { if (this.isDarkMode) { this.clearMode(); } else { this.setDark(); } } } else { if (this.isSysDarkPrefer) { this.setLight(); } else { this.setDark(); } } this.updateMermaid(); } /* flipMode() */ } /* ModeToggle */ let toggle = new ModeToggle(); $(".mode-toggle").click(function() { toggle.flipMode(); }); </script> </span></div></div><div id="topbar-wrapper" class="row justify-content-center topbar-down"><div id="topbar" class="col-11 d-flex h-100 align-items-center justify-content-between"> <span id="breadcrumb"> <span> <a href="/"> Home </a> </span> <span>End-to-End Testing for AI Agents with LLM as a Judge</span> </span> <i id="sidebar-trigger" class="fas fa-bars fa-fw"></i><div id="topbar-title"> Post</div><i id="search-trigger" class="fas fa-search fa-fw"></i> <span id="search-wrapper" class="align-items-center"> <i class="fas fa-search fa-fw"></i> <input class="form-control" id="search-input" type="search" aria-label="search" autocomplete="off" placeholder="Search..."> <i class="fa fa-times-circle fa-fw" id="search-cleaner"></i> </span> <span id="search-cancel" >Cancel</span></div></div><div id="main-wrapper"><div id="main"><div class="row"><div id="post-wrapper" class="col-12 col-lg-11 col-xl-8"><div class="post pl-1 pr-1 pl-sm-2 pr-sm-2 pl-md-4 pr-md-4"><h1 data-toc-skip>End-to-End Testing for AI Agents with LLM as a Judge</h1><div class="post-meta text-muted d-flex flex-column"><div> <span class="semi-bold"> Francois Toubol </span> on <span class="timeago " data-toggle="tooltip" data-placement="bottom" title="Thu, Dec 25, 2025, 8:00 PM -0600" >Dec 25, 2025<i class="unloaded">2025-12-25T20:00:00-06:00</i> </span></div><div> <span> Updated <span class="timeago lastmod" data-toggle="tooltip" data-placement="bottom" title="Sun, Dec 28, 2025, 5:38 PM -0700" >Dec 28, 2025<i class="unloaded">2025-12-28T18:38:17-06:00</i> </span> </span> <span class="readtime" data-toggle="tooltip" data-placement="bottom" title="2323 words">12 min read</span></div></div><div class="post-content"><h2 id="the-testing-challenge">The Testing Challenge</h2><p>AI agents are becoming critical infrastructure. From customer support to data analysis, organizations are deploying agents that make decisions, call APIs, and generate responses autonomously. But this raises a fundamental question: <strong>how do you test something that produces non-deterministic outputs?</strong></p><p>Traditional software testing relies on deterministic assertions. Given input X, expect output Y. But when an LLM is at the core of your system, the same prompt can produce semantically equivalent but textually different responses every time. Your agent might say “You have 3 properties” today and “I found three listings for you” tomorrow. Both are correct, but <code class="language-plaintext highlighter-rouge">assertEqual</code> will fail.</p><p>This post introduces the <strong>LLM-as-judge pattern</strong>, a technique we developed to solve this problem. We’ll walk through how we built an end-to-end testing framework for our AI agent that uses a separate, more capable LLM to evaluate responses, enabling reliable automated testing of inherently non-deterministic systems.</p><h2 id="what-is-an-ai-agent">What is an AI Agent?</h2><p>Before diving into testing, let’s establish what we mean by an “agent.” At its core, an AI agent consists of three components:</p><h3 id="1-the-harness-system-prompt">1. The Harness (System Prompt)</h3><p>The harness defines the agent’s personality, constraints, and context. It’s the system prompt that shapes how the LLM behaves, what it knows about itself, and what rules it should follow.</p><p>In production, this isn’t a simple paragraph. Our system prompt is a structured document spanning several hundred tokens, organized into sections:</p><div class="language-xml highlighter-rouge"><div class="code-header"> <span text-data=" XML "><i class="fa-fw fas fa-code small"></i></span> <button aria-label="copy" title-succeed="Copied!"><i class="far fa-clipboard"></i></button></div><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
33
34
35
36
37
38
39
40
41
42
43
44
45
</pre><td class="rouge-code"><pre><span class="nt">&lt;identity&gt;</span>
You are Neyoba, the AI revenue management assistant for Beyond...
<span class="nt">&lt;/identity&gt;</span>

<span class="nt">&lt;role&gt;</span>
Help users make smarter pricing and performance decisions. Surface insights,
explain data clearly, and guide users to appropriate actions...
<span class="nt">&lt;/role&gt;</span>

<span class="nt">&lt;constraints&gt;</span>
- Cannot process file uploads or generate downloadable files
- If listing lacks dynamic pricing, recommend enabling it first
- Never suggest disabling Beyond pricing
- Never use internal terminology...
<span class="nt">&lt;/constraints&gt;</span>

<span class="nt">&lt;formatting&gt;</span>
- Links: inline HTML with target="_blank"
- Structure: bold headers, short paragraphs, markdown headings
- Data: use tables for comparisons...
<span class="nt">&lt;/formatting&gt;</span>

<span class="nt">&lt;localization&gt;</span>
- Spanish: formal "usted" unless user uses "tú"
- Portuguese: Brazilian conventions
- Dates/currency/spelling: match user's region...
<span class="nt">&lt;/localization&gt;</span>

<span class="nt">&lt;response-behavior&gt;</span>
  <span class="nt">&lt;ambiguous-questions&gt;</span>
    For broad questions ("how am I doing?"):
    1. State your interpretation
    2. Deliver most actionable insight first
    3. Offer 2-3 specific follow-ups
  <span class="nt">&lt;/ambiguous-questions&gt;</span>
  
  <span class="nt">&lt;pacing-questions&gt;</span>...<span class="nt">&lt;/pacing-questions&gt;</span>
  <span class="nt">&lt;clarifications&gt;</span>...<span class="nt">&lt;/clarifications&gt;</span>
<span class="nt">&lt;/response-behavior&gt;</span>

<span class="nt">&lt;tool-usage&gt;</span>
- Never call get_listing_details_by_id in loops
- For 50+ listings, use market-level aggregates
- Combine tools: get_listings for IDs, then get_bookings for details...
<span class="nt">&lt;/tool-usage&gt;</span>
</pre></table></code></div></div><p>The prompt covers identity, role definition, hard constraints, formatting rules, localization preferences, response length guidelines, multi-turn conversation handling, and tool usage patterns. This level of detail is essential for consistent, production-quality agent behavior.</p><p>You can explore our full production system prompt on <a href="https://smith.langchain.com/prompts/sage-cs-chat--system-setup?organizationId=ff257759-ce3a-45f6-afb1-515aaef31216">LangSmith Prompt Hub</a>.</p><h3 id="2-the-reasoning-loop-react-pattern">2. The Reasoning Loop (ReAct Pattern)</h3><p>The agent uses a reasoning loop to decide what to do next. We implement the ReAct (Reasoning + Acting) pattern via LangGraph, where the LLM can:</p><ul><li>Reason about the user’s request<li>Decide which tool to call<li>Process tool results<li>Generate a final response</ul><h3 id="3-the-tools-mcp-server">3. The Tools (MCP Server)</h3><p>Tools give the agent real capabilities. We use the Model Context Protocol (MCP) to expose tools via an SSE server. The agent can call tools like <code class="language-plaintext highlighter-rouge">get_listings</code>, <code class="language-plaintext highlighter-rouge">get_bookings</code>, or <code class="language-plaintext highlighter-rouge">get_revenue_data</code> to fetch real information from our database.</p><p>Here’s how these components interact in a typical request:</p><pre><code class="language-mermaid">sequenceDiagram
    participant User
    participant Agent as ReactAgent
    participant LLM as LLM (GPT-5 Mini)
    participant MCP as MCP Server
    participant DB as Database

    User-&gt;&gt;Agent: "What are my listings?"
    Agent-&gt;&gt;LLM: System prompt + User message
    LLM-&gt;&gt;Agent: Tool call: get_listings(token)
    Agent-&gt;&gt;MCP: Execute tool via SSE
    MCP-&gt;&gt;DB: Query listings
    DB-&gt;&gt;MCP: Listing data
    MCP-&gt;&gt;Agent: Tool result
    Agent-&gt;&gt;LLM: Tool result + context
    LLM-&gt;&gt;Agent: Final response
    Agent-&gt;&gt;User: "You have 3 listings..."
</code></pre><h2 id="why-traditional-assertions-fail">Why Traditional Assertions Fail</h2><p>Consider this test case: “Ask the agent for a list of properties and verify it returns them correctly.”</p><p>With traditional testing, you might write:</p><div class="language-python highlighter-rouge"><div class="code-header"> <span text-data=" Python "><i class="fa-fw fas fa-code small"></i></span> <button aria-label="copy" title-succeed="Copied!"><i class="far fa-clipboard"></i></button></div><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
</pre><td class="rouge-code"><pre><span class="k">def</span> <span class="nf">test_get_listings</span><span class="p">():</span>
    <span class="n">response</span> <span class="o">=</span> <span class="n">agent</span><span class="p">.</span><span class="n">invoke</span><span class="p">(</span><span class="s">"What are my listings?"</span><span class="p">)</span>
    <span class="k">assert</span> <span class="n">response</span> <span class="o">==</span> <span class="s">"You have 3 listings: Beach House, Mountain Cabin, City Apartment"</span>
</pre></table></code></div></div><p>This test will fail immediately. The agent might respond with:</p><ul><li>“I found 3 properties in your portfolio: Beach House, Mountain Cabin, and City Apartment.”<li>“You have three listings:\n1. Beach House\n2. Mountain Cabin\n3. City Apartment”<li>“Here are your 3 listings: Beach House, City Apartment, Mountain Cabin” (different order)</ul><p>All three responses are correct. They contain the same information. But string matching fails because <strong>LLMs are non-deterministic by design</strong>. Even with temperature=0, subtle variations in tokenization, context, and model state can produce different outputs.</p><p>You could try fuzzy matching or regex, but this quickly becomes unmaintainable. How do you write a regex that matches “3”, “three”, and “3 properties” while rejecting “2 listings”?</p><p>The solution: <strong>let an LLM do the evaluation</strong>.</p><h2 id="the-llm-as-judge-pattern">The LLM-as-Judge Pattern</h2><p>The core insight is simple: if an LLM can generate natural language, another LLM can evaluate it. We use a separate, more capable model as a “judge” to score the agent’s responses against specific criteria.</p><p>Here’s the architecture:</p><pre><code class="language-mermaid">flowchart LR
    subgraph Trigger[Trigger]
        CLI[CLI]
        Web[Web App]
    end

    subgraph Framework[Test Framework]
        Executor[Test Executor]
        DB[(PostgreSQL)]
    end

    subgraph Agent[Agent Under Test]
        RA[ReactAgent]
        LLM1[GPT-5 Mini]
        MCP[MCP Server]
    end

    subgraph Eval[Evaluation]
        Judge[LLM Judge]
        LLM2[Claude Opus 4.5]
    end

    CLI --&gt; Executor
    Web --&gt; Executor
    Executor &lt;--&gt; DB
    Executor --&gt; RA
    RA &lt;--&gt; LLM1
    RA &lt;--&gt; MCP
    Executor --&gt; Judge
    Judge &lt;--&gt; LLM2
</code></pre><p>The judge receives three inputs:</p><ol><li><strong>The original user prompt</strong> - what was asked<li><strong>The agent’s response</strong> - what the agent said<li><strong>Evaluation criteria</strong> - what constitutes a good response</ol><p>It returns:</p><ul><li>A <strong>score</strong> between 0.0 and 1.0<li><strong>Reasoning</strong> explaining the score</ul><p>Here’s a simplified implementation:</p><div class="language-python highlighter-rouge"><div class="code-header"> <span text-data=" Python "><i class="fa-fw fas fa-code small"></i></span> <button aria-label="copy" title-succeed="Copied!"><i class="far fa-clipboard"></i></button></div><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
33
34
35
36
37
38
39
40
41
42
43
44
45
46
47
48
49
50
51
52
53
54
</pre><td class="rouge-code"><pre><span class="n">JUDGE_SYSTEM_PROMPT</span> <span class="o">=</span> <span class="s">"""You are an expert evaluator of AI assistant responses. 
Your task is to score responses based on specific criteria provided by the user.

You must respond with ONLY a JSON object in this exact format:
{"score": 0.85, "reasoning": "Brief explanation of the score"}

The score must be a float between 0.0 and 1.0:
- 1.0 = Perfect response that fully meets all criteria
- 0.8+ = Good response that meets most criteria with minor issues
- 0.6-0.8 = Acceptable response with some missing elements
- 0.4-0.6 = Partial response with significant gaps
- Below 0.4 = Poor response that fails to meet criteria

Be objective and consistent in your evaluation."""</span>


<span class="k">async</span> <span class="k">def</span> <span class="nf">judge_response</span><span class="p">(</span>
    <span class="n">response_content</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span>
    <span class="n">prompt</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span>
    <span class="n">judge_prompt</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span>
    <span class="n">threshold</span><span class="p">:</span> <span class="nb">float</span><span class="p">,</span>
    <span class="n">model</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s">"claude-opus-4-5"</span><span class="p">,</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">JudgeResult</span><span class="p">:</span>
    <span class="n">evaluation_prompt</span> <span class="o">=</span> <span class="sa">f</span><span class="s">"""Evaluate the following AI assistant response.

&lt;user_prompt&gt;
</span><span class="si">{</span><span class="n">prompt</span><span class="si">}</span><span class="s">
&lt;/user_prompt&gt;

&lt;ai_response&gt;
</span><span class="si">{</span><span class="n">response_content</span><span class="si">}</span><span class="s">
&lt;/ai_response&gt;

&lt;evaluation_criteria&gt;
</span><span class="si">{</span><span class="n">judge_prompt</span><span class="si">}</span><span class="s">
&lt;/evaluation_criteria&gt;"""</span>

    <span class="n">client</span> <span class="o">=</span> <span class="n">anthropic</span><span class="p">.</span><span class="n">AsyncAnthropic</span><span class="p">()</span>
    <span class="n">message</span> <span class="o">=</span> <span class="k">await</span> <span class="n">client</span><span class="p">.</span><span class="n">messages</span><span class="p">.</span><span class="n">create</span><span class="p">(</span>
        <span class="n">model</span><span class="o">=</span><span class="n">model</span><span class="p">,</span>
        <span class="n">max_tokens</span><span class="o">=</span><span class="mi">500</span><span class="p">,</span>
        <span class="n">system</span><span class="o">=</span><span class="n">JUDGE_SYSTEM_PROMPT</span><span class="p">,</span>
        <span class="n">messages</span><span class="o">=</span><span class="p">[{</span><span class="s">"role"</span><span class="p">:</span> <span class="s">"user"</span><span class="p">,</span> <span class="s">"content"</span><span class="p">:</span> <span class="n">evaluation_prompt</span><span class="p">}],</span>
    <span class="p">)</span>
    
    <span class="n">result</span> <span class="o">=</span> <span class="n">json</span><span class="p">.</span><span class="n">loads</span><span class="p">(</span><span class="n">message</span><span class="p">.</span><span class="n">content</span><span class="p">[</span><span class="mi">0</span><span class="p">].</span><span class="n">text</span><span class="p">)</span>
    <span class="n">passed</span> <span class="o">=</span> <span class="n">result</span><span class="p">[</span><span class="s">"score"</span><span class="p">]</span> <span class="o">&gt;=</span> <span class="n">threshold</span>
    
    <span class="k">return</span> <span class="n">JudgeResult</span><span class="p">(</span>
        <span class="n">passed</span><span class="o">=</span><span class="n">passed</span><span class="p">,</span>
        <span class="n">score</span><span class="o">=</span><span class="n">result</span><span class="p">[</span><span class="s">"score"</span><span class="p">],</span>
        <span class="n">threshold</span><span class="o">=</span><span class="n">threshold</span><span class="p">,</span>
        <span class="n">reasoning</span><span class="o">=</span><span class="n">result</span><span class="p">[</span><span class="s">"reasoning"</span><span class="p">],</span>
    <span class="p">)</span>
</pre></table></code></div></div><h2 id="the-model-hierarchy-strategy">The Model Hierarchy Strategy</h2><p>A key design decision: <strong>the judge should be more capable than the agent being tested</strong>.</p><p>Our agent uses GPT-5 Mini for production, optimizing for cost and latency. But for evaluation, we use Claude Opus, a more capable model with stronger reasoning abilities.</p><p>This works because:</p><ol><li><strong>The judge doesn’t need domain knowledge</strong> - it only evaluates whether criteria are met<li><strong>Evaluation is simpler than generation</strong> - it’s easier to judge quality than to produce it<li><strong>Cost is acceptable for testing</strong> - you run tests less frequently than production queries</ol><p>Configuration example:</p><div class="language-python highlighter-rouge"><div class="code-header"> <span text-data=" Python "><i class="fa-fw fas fa-code small"></i></span> <button aria-label="copy" title-succeed="Copied!"><i class="far fa-clipboard"></i></button></div><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
</pre><td class="rouge-code"><pre><span class="n">config</span> <span class="o">=</span> <span class="p">{</span>
    <span class="s">"agent"</span><span class="p">:</span> <span class="p">{</span>
        <span class="s">"model"</span><span class="p">:</span> <span class="s">"openai:gpt-5-mini"</span><span class="p">,</span>
    <span class="p">},</span>
    <span class="s">"judge"</span><span class="p">:</span> <span class="p">{</span>
        <span class="s">"model"</span><span class="p">:</span> <span class="s">"anthropic:claude-opus-4-5"</span><span class="p">,</span>
        <span class="s">"default_threshold"</span><span class="p">:</span> <span class="mf">0.7</span><span class="p">,</span>
    <span class="p">},</span>
<span class="p">}</span>
</pre></table></code></div></div><h2 id="the-scoring-rubric-approach">The Scoring Rubric Approach</h2><p>The judge uses a consistent scoring rubric:</p><div class="table-wrapper"><table><thead><tr><th>Score<th>Meaning<tbody><tr><td>1.0<td>Perfect response that fully meets all criteria<tr><td>0.8+<td>Good response with minor issues<tr><td>0.6-0.8<td>Acceptable with some missing elements<tr><td>0.4-0.6<td>Partial response with significant gaps<tr><td>&lt; 0.4<td>Poor response that fails to meet criteria</table></div><p>Each test case defines its own evaluation criteria via a <code class="language-plaintext highlighter-rouge">judge_prompt</code>. This allows you to be specific about what matters for each scenario:</p><div class="language-python highlighter-rouge"><div class="code-header"> <span text-data=" Python "><i class="fa-fw fas fa-code small"></i></span> <button aria-label="copy" title-succeed="Copied!"><i class="far fa-clipboard"></i></button></div><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
</pre><td class="rouge-code"><pre><span class="n">test_case</span> <span class="o">=</span> <span class="p">{</span>
    <span class="s">"name"</span><span class="p">:</span> <span class="s">"get_listing_details"</span><span class="p">,</span>
    <span class="s">"prompt"</span><span class="p">:</span> <span class="s">"What are the details of my Beach House listing?"</span><span class="p">,</span>
    <span class="s">"expected_tools"</span><span class="p">:</span> <span class="p">[</span><span class="s">"get_listing_details"</span><span class="p">],</span>
    <span class="s">"judge_prompt"</span><span class="p">:</span> <span class="s">"""The response should contain:
        - The listing name (Beach House)
        - The property address
        - Current pricing information
        - Occupancy rate or availability status
        The information should be formatted clearly and be easy to read."""</span><span class="p">,</span>
    <span class="s">"judge_threshold"</span><span class="p">:</span> <span class="mf">0.7</span><span class="p">,</span>
<span class="p">}</span>
</pre></table></code></div></div><p>The threshold is configurable per test. Critical paths might require 0.9+, while exploratory features might accept 0.6.</p><h2 id="multi-round-and-agent-revival-testing">Multi-Round and Agent Revival Testing</h2><p>Real conversations aren’t single-turn. Users ask follow-up questions, refer to previous context, and expect the agent to remember what was discussed.</p><p>Our framework supports multi-round testing:</p><div class="language-python highlighter-rouge"><div class="code-header"> <span text-data=" Python "><i class="fa-fw fas fa-code small"></i></span> <button aria-label="copy" title-succeed="Copied!"><i class="far fa-clipboard"></i></button></div><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
14
</pre><td class="rouge-code"><pre><span class="n">test_case</span> <span class="o">=</span> <span class="p">{</span>
    <span class="s">"name"</span><span class="p">:</span> <span class="s">"multi_round_conversation"</span><span class="p">,</span>
    <span class="s">"rounds"</span><span class="p">:</span> <span class="p">[</span>
        <span class="p">{</span>
            <span class="s">"prompt"</span><span class="p">:</span> <span class="s">"What are my listings?"</span><span class="p">,</span>
            <span class="s">"judge_prompt"</span><span class="p">:</span> <span class="s">"Should list all properties"</span><span class="p">,</span>
        <span class="p">},</span>
        <span class="p">{</span>
            <span class="s">"prompt"</span><span class="p">:</span> <span class="s">"What's the occupancy for the first one?"</span><span class="p">,</span>
            <span class="s">"judge_prompt"</span><span class="p">:</span> <span class="s">"Should reference the first listing from previous response and show occupancy data"</span><span class="p">,</span>
        <span class="p">},</span>
    <span class="p">],</span>
    <span class="s">"test_agent_revival"</span><span class="p">:</span> <span class="bp">True</span><span class="p">,</span>
<span class="p">}</span>
</pre></table></code></div></div><p>The <code class="language-plaintext highlighter-rouge">test_agent_revival</code> flag is particularly interesting. It tests whether the agent can:</p><ol><li>Complete round 1<li><strong>Close the session entirely</strong><li>Reopen with the same agent ID<li>Continue the conversation with context preserved</ol><p>This validates that your checkpointing and state persistence work correctly, catching bugs that would only appear when users return to conversations after server restarts.</p><h2 id="complementary-validators">Complementary Validators</h2><p>The LLM judge is powerful but not the only validator. We combine it with deterministic checks:</p><h3 id="tool-validator">Tool Validator</h3><p>Verify that expected MCP tools were called:</p><div class="language-python highlighter-rouge"><div class="code-header"> <span text-data=" Python "><i class="fa-fw fas fa-code small"></i></span> <button aria-label="copy" title-succeed="Copied!"><i class="far fa-clipboard"></i></button></div><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
</pre><td class="rouge-code"><pre><span class="k">def</span> <span class="nf">validate_tools</span><span class="p">(</span><span class="n">response</span><span class="p">,</span> <span class="n">expected_tools</span><span class="p">):</span>
    <span class="n">called_tools</span> <span class="o">=</span> <span class="n">extract_tool_calls</span><span class="p">(</span><span class="n">response</span><span class="p">)</span>
    <span class="n">missing</span> <span class="o">=</span> <span class="p">[</span><span class="n">t</span> <span class="k">for</span> <span class="n">t</span> <span class="ow">in</span> <span class="n">expected_tools</span> <span class="k">if</span> <span class="n">t</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">called_tools</span><span class="p">]</span>
    <span class="k">return</span> <span class="n">ToolValidationResult</span><span class="p">(</span>
        <span class="n">passed</span><span class="o">=</span><span class="nb">len</span><span class="p">(</span><span class="n">missing</span><span class="p">)</span> <span class="o">==</span> <span class="mi">0</span><span class="p">,</span>
        <span class="n">expected_tools</span><span class="o">=</span><span class="n">expected_tools</span><span class="p">,</span>
        <span class="n">called_tools</span><span class="o">=</span><span class="n">called_tools</span><span class="p">,</span>
        <span class="n">missing_tools</span><span class="o">=</span><span class="n">missing</span><span class="p">,</span>
    <span class="p">)</span>
</pre></table></code></div></div><p>This is deterministic. If the test expects <code class="language-plaintext highlighter-rouge">get_listings</code> to be called and it wasn’t, that’s a clear failure regardless of what the response says.</p><h3 id="database-validator">Database Validator</h3><p>Verify that conversations and messages are persisted correctly:</p><div class="language-python highlighter-rouge"><div class="code-header"> <span text-data=" Python "><i class="fa-fw fas fa-code small"></i></span> <button aria-label="copy" title-succeed="Copied!"><i class="far fa-clipboard"></i></button></div><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
</pre><td class="rouge-code"><pre><span class="k">def</span> <span class="nf">validate_database</span><span class="p">(</span><span class="n">agent_id</span><span class="p">,</span> <span class="n">verify_conversation</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> <span class="n">verify_messages</span><span class="o">=</span><span class="bp">True</span><span class="p">):</span>
    <span class="k">if</span> <span class="n">verify_conversation</span><span class="p">:</span>
        <span class="n">conversation</span> <span class="o">=</span> <span class="n">get_conversation_by_id</span><span class="p">(</span><span class="n">agent_id</span><span class="p">)</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="n">conversation</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">DatabaseValidationResult</span><span class="p">(</span><span class="n">passed</span><span class="o">=</span><span class="bp">False</span><span class="p">,</span> <span class="n">message</span><span class="o">=</span><span class="s">"Conversation not found"</span><span class="p">)</span>
    
    <span class="k">if</span> <span class="n">verify_messages</span><span class="p">:</span>
        <span class="n">messages</span> <span class="o">=</span> <span class="n">get_messages_by_thread</span><span class="p">(</span><span class="n">agent_id</span><span class="p">)</span>
        <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">messages</span><span class="p">)</span> <span class="o">&lt;</span> <span class="mi">2</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">DatabaseValidationResult</span><span class="p">(</span><span class="n">passed</span><span class="o">=</span><span class="bp">False</span><span class="p">,</span> <span class="n">message</span><span class="o">=</span><span class="s">"Expected at least 2 messages"</span><span class="p">)</span>
    
    <span class="k">return</span> <span class="n">DatabaseValidationResult</span><span class="p">(</span><span class="n">passed</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
</pre></table></code></div></div><h2 id="the-follow-up-detection-pattern">The Follow-up Detection Pattern</h2><p>Here’s an edge case that caused many false failures: sometimes the agent asks for clarification instead of answering.</p><p>User: “What’s the occupancy?” Agent: “I’d be happy to help! Which listing would you like to see the occupancy for?”</p><p>This is valid agent behavior, but it fails a test expecting occupancy data. The solution: use another LLM to detect clarification requests and auto-respond.</p><div class="language-python highlighter-rouge"><div class="code-header"> <span text-data=" Python "><i class="fa-fw fas fa-code small"></i></span> <button aria-label="copy" title-succeed="Copied!"><i class="far fa-clipboard"></i></button></div><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
</pre><td class="rouge-code"><pre><span class="k">async</span> <span class="k">def</span> <span class="nf">check_needs_followup</span><span class="p">(</span><span class="n">ai_response</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span> <span class="n">original_prompt</span><span class="p">:</span> <span class="nb">str</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">tuple</span><span class="p">[</span><span class="nb">bool</span><span class="p">,</span> <span class="nb">str</span><span class="p">]:</span>
    <span class="n">check_prompt</span> <span class="o">=</span> <span class="sa">f</span><span class="s">"""Analyze this AI response to determine if it's asking for clarification.

&lt;original_user_request&gt;
</span><span class="si">{</span><span class="n">original_prompt</span><span class="si">}</span><span class="s">
&lt;/original_user_request&gt;

&lt;ai_response&gt;
</span><span class="si">{</span><span class="n">ai_response</span><span class="si">}</span><span class="s">
&lt;/ai_response&gt;

If the AI is asking for clarification, respond with:
{{"needs_followup": true, "followup_response": "A reasonable response providing the needed clarification"}}

If the AI provided a substantive answer, respond with:
{{"needs_followup": false, "followup_response": ""}}"""</span>

    <span class="n">client</span> <span class="o">=</span> <span class="n">anthropic</span><span class="p">.</span><span class="n">AsyncAnthropic</span><span class="p">()</span>
    <span class="n">message</span> <span class="o">=</span> <span class="k">await</span> <span class="n">client</span><span class="p">.</span><span class="n">messages</span><span class="p">.</span><span class="n">create</span><span class="p">(</span>
        <span class="n">model</span><span class="o">=</span><span class="s">"claude-sonnet-4-20250514"</span><span class="p">,</span>
        <span class="n">max_tokens</span><span class="o">=</span><span class="mi">300</span><span class="p">,</span>
        <span class="n">messages</span><span class="o">=</span><span class="p">[{</span><span class="s">"role"</span><span class="p">:</span> <span class="s">"user"</span><span class="p">,</span> <span class="s">"content"</span><span class="p">:</span> <span class="n">check_prompt</span><span class="p">}],</span>
    <span class="p">)</span>
    
    <span class="n">result</span> <span class="o">=</span> <span class="n">json</span><span class="p">.</span><span class="n">loads</span><span class="p">(</span><span class="n">message</span><span class="p">.</span><span class="n">content</span><span class="p">[</span><span class="mi">0</span><span class="p">].</span><span class="n">text</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">result</span><span class="p">[</span><span class="s">"needs_followup"</span><span class="p">],</span> <span class="n">result</span><span class="p">[</span><span class="s">"followup_response"</span><span class="p">]</span>
</pre></table></code></div></div><p>The executor uses this to automatically continue conversations up to a maximum number of follow-ups, ensuring tests evaluate the final answer rather than intermediate clarifications.</p><h2 id="democratizing-test-creation-the-web-ui">Democratizing Test Creation: The Web UI</h2><p>One final insight: <strong>testing shouldn’t be engineers-only</strong>.</p><p><img data-proofer-ignore data-src="/assets/images/posts/llm-as-judge/e2e-tests-web-ui.png" alt="E2E Test Web UI" /> <em>The web UI shows test results with judge scores and reasoning for each round</em></p><p>Product managers often understand user intent better than engineers. They know what users actually ask and what good answers look like. But if tests live in code, PMs can’t contribute.</p><p>Our solution: store test cases in PostgreSQL and build a web UI for creating and managing them.</p><p><strong>Why a database instead of config files:</strong></p><ul><li>Non-engineers can participate without code changes<li>Test history and results are preserved<li>Easy to build dashboards and analytics<li>Supports the web UI and CLI equally</ul><p><strong>Dual interfaces:</strong></p><ul><li><strong>CLI</strong> for engineers and CI pipelines<li><strong>Web App</strong> for everyone else</ul><p><strong>Benefits:</strong></p><ul><li>PMs can define test scenarios based on real user questions<li>Faster iteration on test coverage<li>No pull requests required to add new tests<li>Visibility into test results for the whole team</ul><p>The web UI lets anyone create a test case by filling in:</p><ul><li>A name and description<li>The user prompt<li>Expected tools (optional)<li>Judge evaluation criteria<li>Pass/fail threshold</ul><p>Results are stored and displayed in a dashboard, making it easy to track agent quality over time.</p><h2 id="conclusion">Conclusion</h2><p>Testing AI agents requires a fundamental shift in approach. Traditional assertions fail because LLM outputs are non-deterministic by design. The LLM-as-judge pattern solves this by using a more capable model to evaluate responses against semantic criteria rather than exact matches.</p><p><strong>Key takeaways:</strong></p><ol><li><p><strong>Use a superior model as judge</strong> - Claude Opus evaluating GPT-5 Mini responses works well because evaluation is simpler than generation.</p><li><p><strong>Combine LLM evaluation with deterministic checks</strong> - Tool validation and database verification catch issues that semantic evaluation might miss.</p><li><p><strong>Test multi-round conversations</strong> - Real users have follow-up questions. Test that context persists correctly.</p><li><p><strong>Handle clarification loops</strong> - Agents asking for clarification is valid behavior. Detect it and auto-respond to get to the actual answer.</p><li><p><strong>Democratize test creation</strong> - Store tests in a database with a web UI so non-engineers can contribute. They often understand user intent better than we do.</p></ol><p>The result is a testing framework that catches real regressions while tolerating the natural variation in LLM outputs. Our agents are more reliable, and our whole team can contribute to quality.</p></div><div class="post-tail-wrapper text-muted"><div class="post-tail-bottom d-flex justify-content-between align-items-center mt-3 pt-5 pb-2"><div class="license-wrapper"> This post is licensed under <a href="https://creativecommons.org/licenses/by/4.0/"> CC BY 4.0 </a> by the author.</div><div class="share-wrapper"> <span class="share-label text-muted mr-1">Share</span> <span class="share-icons"> <a href="https://twitter.com/intent/tweet?text=End-to-End Testing for AI Agents with LLM as a Judge - Engineering<br/> @ Beyond &url=https://1024inc.github.io/engineering-blog//posts/llm-as-judge-e2e-testing-ai-agents/" data-toggle="tooltip" data-placement="top" title="Twitter" target="_blank" rel="noopener" aria-label="Twitter"> <i class="fa-fw fab fa-twitter"></i> </a> <a href="https://www.facebook.com/sharer/sharer.php?title=End-to-End Testing for AI Agents with LLM as a Judge - Engineering<br/> @ Beyond &u=https://1024inc.github.io/engineering-blog//posts/llm-as-judge-e2e-testing-ai-agents/" data-toggle="tooltip" data-placement="top" title="Facebook" target="_blank" rel="noopener" aria-label="Facebook"> <i class="fa-fw fab fa-facebook-square"></i> </a> <a href="https://telegram.me/share?text=End-to-End Testing for AI Agents with LLM as a Judge - Engineering<br/> @ Beyond &url=https://1024inc.github.io/engineering-blog//posts/llm-as-judge-e2e-testing-ai-agents/" data-toggle="tooltip" data-placement="top" title="Telegram" target="_blank" rel="noopener" aria-label="Telegram"> <i class="fa-fw fab fa-telegram"></i> </a> <i id="copy-link" class="fa-fw fas fa-link small" data-toggle="tooltip" data-placement="top" title="Copy link" title-succeed="Link copied successfully!"> </i> </span></div></div></div></div></div><div id="panel-wrapper" class="col-xl-3 pl-2 text-muted topbar-down"><div class="access"><div id="access-lastmod" class="post"> <span>Recent Update</span><ul class="post-content pl-0 pb-1 ml-1 mt-2"><li><a href="/posts/llm-as-judge-e2e-testing-ai-agents/">End-to-End Testing for AI Agents with LLM as a Judge</a><li><a href="/posts/mocking-requests-to-third-party-services/">Mocking requests to third party services</a><li><a href="/posts/tools-to-backfill-large-postgresql-tables-pt1/">Tools to backfill large PostgreSQL tables (part 1)</a><li><a href="/posts/post-mortems-intro/">Post-mortems intro</a><li><a href="/posts/empathy-and-compassion-in-communication-part-1/">Empathy and Compassion in Communication (part 1)</a></ul></div></div><script src="https://cdn.jsdelivr.net/gh/afeld/bootstrap-toc@1.0.1/dist/bootstrap-toc.min.js"></script><div id="toc-wrapper" class="pl-0 pr-4 mb-5"> <span class="pl-3 pt-2 mb-2">Contents</span><nav id="toc" data-toggle="toc"></nav></div></div></div><div class="row"><div class="col-12 col-lg-11 col-xl-8"><div id="post-extend-wrapper" class="pl-1 pr-1 pl-sm-2 pr-sm-2 pl-md-4 pr-md-4"><div id="related-posts" class="mt-5 mb-2 mb-sm-4"><h3 class="pt-2 mt-1 mb-4 ml-1" data-toc-skip>Further Reading</h3><div class="card-deck mb-4"><div class="card"> <a href="/posts/mocking-requests-to-third-party-services/"><div class="card-body"> <span class="timeago small" >Aug 9, 2022<i class="unloaded">2022-08-09T19:00:00-05:00</i> </span><h3 class="pt-0 mt-1 mb-3" data-toc-skip>Mocking requests to third party services</h3><div class="text-muted small"><p> Third party services are frequently used to extend the functionality of our product/service, especially in cases where what is needed grows beyond the limits of our industry or purpose. Once an int...</p></div></div></a></div><div class="card"> <a href="/posts/tools-to-backfill-large-postgresql-tables-pt1/"><div class="card-body"> <span class="timeago small" >May 31, 2022<i class="unloaded">2022-05-31T19:00:00-05:00</i> </span><h3 class="pt-0 mt-1 mb-3" data-toc-skip>Tools to backfill large PostgreSQL tables (part 1)</h3><div class="text-muted small"><p> At Beyond we are mainly using PostgreSQL databases for our services, consisting of over a hundred tables. With some of the largest ones we’ve already had a few close calls as they were reaching the...</p></div></div></a></div><div class="card"> <a href="/posts/empathy-and-compassion-in-communication-part-2/"><div class="card-body"> <span class="timeago small" >Apr 11, 2022<i class="unloaded">2022-04-11T23:00:00-05:00</i> </span><h3 class="pt-0 mt-1 mb-3" data-toc-skip>Empathy and Compassion in Communication (part 2)</h3><div class="text-muted small"><p> Effective engineers are strong problem solvers. Great engineers have also become inspiring leaders and master communicators. Whether you want to take your career to the next level, or simply aspi...</p></div></div></a></div></div></div><div class="post-navigation d-flex justify-content-between"> <a href="/posts/mocking-requests-to-third-party-services/" class="btn btn-outline-primary" prompt="Older"><p>Mocking requests to third party services</p></a> <span class="btn btn-outline-primary disabled" prompt="Newer"><p>-</p></span></div></div></div></div><footer class="d-flex w-100 justify-content-center"><div class="d-flex justify-content-between align-items-center"><div class="footer-left"><p class="mb-0"> © 2026 <a href="https://twitter.com/BeyondPricing">Beyond Engineering</a>. <span data-toggle="tooltip" data-placement="top" title="Except where otherwise noted, the blog posts on this site are licensed under the Creative Commons Attribution 4.0 International (CC BY 4.0) License by the author.">Some rights reserved.</span></p></div><div class="footer-right"><p class="mb-0"> Powered by <a href="https://jekyllrb.com" target="_blank" rel="noopener">Jekyll</a> with <a href="https://github.com/cotes2020/jekyll-theme-chirpy" target="_blank" rel="noopener">Chirpy</a> theme.</p></div></div></footer></div><div id="search-result-wrapper" class="d-flex justify-content-center unloaded"><div class="col-12 col-sm-11 post-content"><div id="search-hints"><h4 class="text-muted mb-4">Trending Tags</h4></div><div id="search-results" class="d-flex flex-wrap justify-content-center text-muted mt-3"></div></div></div></div><script src="https://cdn.jsdelivr.net/npm/mermaid@8/dist/mermaid.min.js"></script> <script> $(function() { let initTheme = "default"; if ($("html[mode=dark]").length > 0 || ($("html[mode]").length == 0 && window.matchMedia("(prefers-color-scheme: dark)").matches ) ) { initTheme = "dark"; } let mermaidConf = { theme: initTheme /* <default|dark|forest|neutral> */ }; /* Markdown converts to HTML */ $("pre").has("code.language-mermaid").each(function() { let svgCode = $(this).children().html(); $(this).addClass("unloaded"); $(this).after(`<div class=\"mermaid\">${svgCode}</div>`); }); mermaid.initialize(mermaidConf); }); </script><div id="mask"></div><a id="back-to-top" href="#" aria-label="back-to-top" class="btn btn-lg btn-box-shadow" role="button"> <i class="fas fa-angle-up"></i> </a> <script src="https://cdn.jsdelivr.net/npm/simple-jekyll-search@1.10.0/dest/simple-jekyll-search.min.js"></script> <script> SimpleJekyllSearch({ searchInput: document.getElementById('search-input'), resultsContainer: document.getElementById('search-results'), json: '/assets/js/data/search.json', searchResultTemplate: '<div class="pl-1 pr-1 pl-sm-2 pr-sm-2 pl-lg-4 pr-lg-4 pl-xl-0 pr-xl-0"> <a href="{url}">{title}</a><div class="post-meta d-flex flex-column flex-sm-row text-muted mt-1 mb-1"> {categories} {tags}</div><p>{snippet}</p></div>', noResultsText: '<p class="mt-5">Oops! No result founds.</p>', templateMiddleware: function(prop, value, template) { if (prop === 'categories') { if (value === '') { return `${value}`; } else { return `<div class="mr-sm-4"><i class="far fa-folder fa-fw"></i>${value}</div>`; } } if (prop === 'tags') { if (value === '') { return `${value}`; } else { return `<div><i class="fa fa-tag fa-fw"></i>${value}</div>`; } } } }); </script> <script src="https://cdn.jsdelivr.net/combine/npm/lozad/dist/lozad.min.js,npm/magnific-popup@1/dist/jquery.magnific-popup.min.js,npm/clipboard@2/dist/clipboard.min.js"></script> <script defer src="/assets/js/dist/post.min.js"></script> <script src="https://cdn.jsdelivr.net/combine/npm/popper.js@1.16.1,npm/bootstrap@4/dist/js/bootstrap.min.js"></script> <script defer src="/app.js"></script> <script defer src="https://www.googletagmanager.com/gtag/js?id="></script> <script> document.addEventListener("DOMContentLoaded", function(event) { window.dataLayer = window.dataLayer || []; function gtag(){dataLayer.push(arguments);} gtag('js', new Date()); gtag('config', ''); }); </script>
